[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "I love every story. The real story broadens my views to change myself. Dealing with data is like a treasure hunter of stories. I think it is a fascinating job to find hidden stories and their effect on people’s minds."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Introduction",
    "section": "Education",
    "text": "Education\nBrigham Young University - Idaho | Rexburg, ID  * Bachelor of Mathematics in Data Science * Minor | Statistics * Certificate | Database, Web Fronted   Relevant Skills |  Machine Learning, Business Analysis, Data Wrangling/Visualization, APIs, Time Series Analysis, Multiple Linear Regression, Database Management, Dynamic Websites"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Introduction",
    "section": "Experience",
    "text": "Experience\n\nLead Data Visualization Specialist | Sep 2023 - Present\nSRR Curriculum Processor | Sep 2022 - Sep 2023\nOperator & Manager | Jan 2018 - Dec 2021"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Introduction",
    "section": "Skills",
    "text": "Skills\n\nPower BI\nSQL\nPython\nR\nDatabricks\nGitHub\nHTML\nCSS\nJavaScript"
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "[Project2. Are we missing JSON on our flight?]",
    "section": "",
    "text": "Course DS 250 Gakyeong Bae\n\n\nWhen using airport, it seems natual to wait airplan for a while without knowing the reason. People try to have more time spaces between flights because of late flight. I can figure out why the flight is late so often using the date. I found the raw and not defined date in the frame and filled out variables so that there have no missing values. when it comes to analyze the data, I reorganize data into right columns so that I expect the reason of late flight and the time of late flight with understading.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nairport_code\nnum_of_flights_total\nnum_of_delays_total\nminutes_delayed_total\nportion_delay\naverage_min_delay\n\n\n\n\nATL\n4430047\n902443\n53983926\n26.6177\n130.614\n\n\nDEN\n2513974\n468519\n25173381\n24.5232\n116.501\n\n\nIAD\n851571\n168467\n10283478\n25.7612\n132.428\n\n\nORD\n3597588\n830825\n56356129\n30.2032\n145.955\n\n\nSAN\n917862\n175132\n8276248\n25.0415\n103.492\n\n\nSFO\n1630945\n425604\n26550493\n34.4008\n134.33\n\n\nSLC\n1403384\n205160\n10123371\n19.0164\n108.713\n\n\n\n\ntechnic note : understanding to use lambda funcion so that make a new columns.\n\npor_delay1 = (delay_info\n    .assign(\n        portion_delay = lambda x : \n        x.num_of_delays_total / x.num_of_flights_total,\n        average_min_delay = lambda x :\n        x.minutes_delayed_total  / x.num_of_delays_total / 60 )\n ### 2. Do you think the worst month to travel in summer? #### (Define worst months on each airport and show the chart of it) ###### people usually have their vacation in summer, so people might think summer have more delayed flight. However, the results shows that december is the worst month to travel because flight might delayed in december very often at any airport. Unexpectedly, June is the second proportion of number of delayed flights.\n\n\ntechnic note : understanding to order list of items and to replaces the axis with different format.\n\n.encode(\n        x =alt.X('month', sort = order_month, title = \"Month\"),\n        y = alt.Y('proportion_delayed', axis = alt.Axis(format= \"%\"), \n        title=\"portion_delayed\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nairport_code\nnum_of_delays_weather\nnum_of_delays_late_aircraft\n\n\n\n\nATL\n245.265\n1759.97\n\n\nDEN\n104.818\n1363.64\n\n\nIAD\n36.3182\n491.189\n\n\nORD\n157.311\n2043.83\n\n\nSAN\n32.7273\n534.856\n\n\nSFO\n78.6136\n940.682\n\n\nSLC\n51.75\n601.902\n\n\n\n\n\n\nairport_code\nlate_plus_weather\nrevise_late_arrive\n\n\n\n\nATL\n773.257\n1231.98\n\n\nDEN\n513.911\n954.551\n\n\nIAD\n183.675\n343.833\n\n\nORD\n770.459\n1430.68\n\n\nSAN\n193.184\n374.399\n\n\nSFO\n360.818\n658.477\n\n\nSLC\n232.32\n421.331\n\n\n\n\n\n\n\n\n\n\nmonth\nnum_of_delays_nas\nnum_of_delays_weather\n\n\n\n\nApril\n1216.4\n76.6104\n\n\nAugust\n1395.75\n111.935\n\n\nDecember\n1563.08\n131.632\n\n\nFebruary\n1408.7\n115.143\n\n\nJanuary\n1468.24\n126.859\n\n\nJuly\n1523.43\n137.195\n\n\nJune\n1610.32\n134.961\n\n\nMarch\n1373.14\n94.1579\n\n\nMay\n1353.77\n88.4156\n\n\nNovember\n1149.79\n63.6883\n\n\nOctober\n1322.21\n64.7051\n\n\nSeptember\n1134.66\n66.8052\n\n\n\n\n\n\nmonth\nrevise_nas\nnas_plus_weather\n\n\n\n\nApril\n486.561\n563.171\n\n\nAugust\n558.301\n670.236\n\n\nDecember\n1016\n1147.63\n\n\nFebruary\n915.656\n1030.8\n\n\nJanuary\n954.358\n1081.22\n\n\nJuly\n609.371\n746.566\n\n\nJune\n644.13\n779.091\n\n\nMarch\n892.544\n986.702\n\n\nMay\n541.506\n629.922\n\n\nNovember\n747.365\n811.053\n\n\nOctober\n859.433\n924.138\n\n\nSeptember\n737.531\n804.336\n\n\n\ntechnic note : understanding to use where function so that condition can be seted either True and False.\nlambda x :\n    np.where(\n        x.month.isin([\"April\",\"May\", \"June\", \"July\", \"August\"]), x.num_of_delays_nas * 0.4 ,\n        x.num_of_delays_nas * 0.65))\n\n\n\n\n\n\n\n\ntechnic note : understanding to arrage the data from the raw data and to make it group for analyzing.\npro_weather2 = (weather_total\n    .assign(pro_weather = lambda x : x. nas_plus_weather + x. late_plus_weather - x. num_of_delays_weather))\n\ngrouped = pro_weather2.groupby('airport_code').sum()\n\n\n\n\n\n\n\n     {\n            \"index\": 2,\n            \"airport_code\": \"IAD\",\n            \"airport_name\": \"\",\n            \"month\": \"January\",\n            \"year\": 2005.0,\n            \"num_of_flights_total\": 12381,\n            \"num_of_delays_carrier\": \"414\",\n            \"num_of_delays_late_aircraft\": 1058,\n            \"num_of_delays_nas\": 895,\n            \"num_of_delays_security\": 4,\n            \"num_of_delays_weather\": 61,\n            \"num_of_delays_total\": 2430,\n            \"minutes_delayed_carrier\": null,\n            \"minutes_delayed_late_aircraft\": 70919,\n            \"minutes_delayed_nas\": 35660.0,\n            \"minutes_delayed_security\": 208,\n            \"minutes_delayed_weather\": 4497,\n            \"minutes_delayed_total\": 134881\n        }\n\n\n\n\nimport pandas as pd\nimport altair as alt\nimport numpy as np\nimport json\n\nurl =\"https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json\"\n\nairport = pd.read_json(url)\n\n# %% missing data\nclean_data = airport\nclean_data.dropna()\nclean_data[\"month\"] = (airport\n    .month.replace('n/a', np.nan)\n    .fillna(method= 'ffill')\n    .replace('Febuary', 'February')\n)\n\nclean_data['num_of_delays_late_aircraft'] = (airport\n    .num_of_delays_late_aircraft\n    .replace(-999, airport.num_of_delays_late_aircraft.mean())\n)\n\nclean_data['num_of_flight_total'] = (airport\n    .num_of_flights_total.replace(np.nan, np.mean)\n)\n# %%\n''' Which airport has the worst delays? How did you choose to define “worst”? As part of your answer include a table that lists the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours, for each airport.'''\n\ndelay_info = clean_data.filter(['airport_code', 'month','num_of_flights_total','num_of_delays_total','minutes_delayed_total'])\n\n#%%\npor_delay = (delay_info\n    .assign(\n        portion_delay = lambda x : \n        x.num_of_delays_total / x.num_of_flights_total,\n        average_min_delay = lambda x :\n        x.minutes_delayed_total  / x.num_of_delays_total / 60 )\n    .groupby('airport_code').sum()\n    .reset_index()\n)\n\nprint(por_delay.to_markdown(index=False))\n#%%\npor_delay1 = (delay_info\n    .assign(\n        portion_delay = lambda x : \n        x.num_of_delays_total / x.num_of_flights_total,\n        average_min_delay = lambda x :\n        x.minutes_delayed_total  / x.num_of_delays_total / 60 )\n    .reset_index()\n)\n\nchart1 = (alt.Chart(por_delay1)\n    .mark_point()\n    .encode(\n        x =alt.X ('portion_delay'), \n        y = alt.Y('average_min_delay'), \n        color = 'airport_code')\n)\n\nchart1.save(\"chart1.png\")\n\n# %%\n'''What is the worst month to fly if you want to avoid delays? Include one chart to help support your answer, with the x-axis ordered by month. You also need to explain and justify how you chose to handle the missing Month data.'''\n#%%\norder_month= ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'Octorber','September', 'October', 'November', 'December']\n#%%\nworst_month = (clean_data\n    .assign(proportion_delayed = lambda x : x.\n    num_of_delays_total / x.num_of_flights_total)\n    .groupby('month').mean()\n    .reset_index()\n)\n\nchart2 = (alt.Chart(worst_month)\n    .mark_bar()\n    .encode(\n        x =alt.X('month', sort = order_month, title = \"Month\"),\n        y = alt.Y('proportion_delayed', axis = alt.Axis(format= \"%\"), \n        title=\"portion_delayed\"))\n    .properties(\n        width = 600, \n        title = \"Worst Month of delaying airplan\")\n)\n\nchart2.save(\"chart2.png\")\n# %%\n'''According to the BTS website the Weather category only accounts for severe weather delays. Other “mild” weather delays are included as part of the NAS category and the Late-Arriving Aircraft category. Calculate the total number of flights delayed by weather (either severe or mild) using these two rules:\n\n1.30% of all delayed flights in the Late-Arriving category are due to weather.\n2. From April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.'''\n\n#%%\nlate_info = (clean_data\n    .groupby('airport_code').mean()\n    .filter(['num_of_delays_weather','num_of_delays_late_aircraft'])\n    .reset_index()\n)\nprint(late_info.to_markdown(index=False))\n# %%\nthirty_late = (clean_data\n    .assign(late_plus_weather = lambda x : \n    x. num_of_delays_late_aircraft * 0.3 + x.num_of_delays_weather)\n    .assign(revise_late_arrive = lambda x : x. num_of_delays_late_aircraft * 0.7) \n    .groupby('airport_code').mean()\n    .filter(['late_plus_weather', 'revise_late_arrive'])\n    .reset_index()\n)\nprint(thirty_late.to_markdown(index=False))\n#%%\nlate_info2 = (clean_data\n    .groupby('month').mean()\n    .filter(['num_of_delays_nas','num_of_delays_weather'])\n    .reset_index()\n   \n)\n\nprint(late_info2.to_markdown(index=False))\n#%%\nforthy_late = (clean_data\n    .assign(revise_nas = lambda x :\n    np.where(\n        x.month.isin([\"April\",\"May\", \"June\", \"July\", \"August\"]), x.num_of_delays_nas * 0.4 ,\n        x.num_of_delays_nas * 0.65))\n    .assign(nas_plus_weather = lambda x :\n        x. revise_nas + x.num_of_delays_weather )\n    .groupby('month').mean()\n    .filter(['revise_nas', 'nas_plus_weather'])\n    .reset_index()\n)\nprint(forthy_late.to_markdown(index=False))\n\n# %%\n'''Create a barplot showing the proportion of all flights that are delayed by weather at each airport. What do you learn from this graph (Careful to handle the missing Late Aircraft data correctly)?'''\n# %%\nweather_total = (clean_data\n    .assign(revise_nas = lambda x :\n    np.where(\n        x.month.isin([\"April\",\"May\", \"June\", \"July\", \"August\"]), x.num_of_delays_nas * 0.4 ,\n        x.num_of_delays_nas * 0.65))\n    .assign(nas_plus_weather = lambda x :\n        x. revise_nas + x.num_of_delays_weather )\n    .assign(late_plus_weather = lambda x : \n    x. num_of_delays_late_aircraft * 0.3 + x.num_of_delays_weather)\n    .assign(revise_late_arrive = lambda x : x. num_of_delays_late_aircraft * 0.7) \n    #.groupby('airport_code').mean()\n    #.filter(['nas_plus_weather', 'late_plus_weather'])\n    #.reset_index()\n)\n#%%\npro_weather2 = (weather_total\n    .assign(pro_weather = lambda x : x. nas_plus_weather + x. late_plus_weather - x. num_of_delays_weather))\n\ngrouped = pro_weather2.groupby('airport_code').sum()\n#%%   \ngrouped['pro_weather_delayed'] = grouped.pro_weather/ grouped.num_of_flights_total \ngrouped = grouped.reset_index()\n# %%\nchart4 = (alt.Chart(grouped)\n    .mark_bar()\n    .encode(\n        x =alt.X ('airport_code', title = \"air_code\"),\n        y = alt.Y('pro_weather_delayed', axis = alt.Axis(format= \"%\"), \n        title=\"portion_delayed_weather\" ))\n        .properties(width = 600, title = \"weatehr delayed airplan\")\n)\nchart4.save(\"chart4.png\")\n# %%\n'''Fix all of the varied NA types in the data to be consistent and save the file back out in the same format that was provided (this file shouldn’t have the missing values replaced with a value). Include one record example from your exported JSON file that has a missing value (No imputation in this file).'''\n#%%\njson_data= airport.to_json(orient='table')\nloading = json.loads(json_data)\nprint(json.dumps(loading, indent = 4))\n\n''' paste JS \n     {\n            \"index\": 2,\n            \"airport_code\": \"IAD\",\n            \"airport_name\": \"\",\n            \"month\": \"January\",\n            \"year\": 2005.0,\n            \"num_of_flights_total\": 12381,\n            \"num_of_delays_carrier\": \"414\",\n            \"num_of_delays_late_aircraft\": 1058,\n            \"num_of_delays_nas\": 895,\n            \"num_of_delays_security\": 4,\n            \"num_of_delays_weather\": 61,\n            \"num_of_delays_total\": 2430,\n            \"minutes_delayed_carrier\": null,\n            \"minutes_delayed_late_aircraft\": 70919,\n            \"minutes_delayed_nas\": 35660.0,\n            \"minutes_delayed_security\": 208,\n            \"minutes_delayed_weather\": 4497,\n            \"minutes_delayed_total\": 134881\n        }\n'''\ncommented python code from .py file"
  },
  {
    "objectID": "project2.html#elevator-pitch",
    "href": "project2.html#elevator-pitch",
    "title": "[Project2. Are we missing JSON on our flight?]",
    "section": "",
    "text": "When using airport, it seems natual to wait airplan for a while without knowing the reason. People try to have more time spaces between flights because of late flight. I can figure out why the flight is late so often using the date. I found the raw and not defined date in the frame and filled out variables so that there have no missing values. when it comes to analyze the data, I reorganize data into right columns so that I expect the reason of late flight and the time of late flight with understading.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nairport_code\nnum_of_flights_total\nnum_of_delays_total\nminutes_delayed_total\nportion_delay\naverage_min_delay\n\n\n\n\nATL\n4430047\n902443\n53983926\n26.6177\n130.614\n\n\nDEN\n2513974\n468519\n25173381\n24.5232\n116.501\n\n\nIAD\n851571\n168467\n10283478\n25.7612\n132.428\n\n\nORD\n3597588\n830825\n56356129\n30.2032\n145.955\n\n\nSAN\n917862\n175132\n8276248\n25.0415\n103.492\n\n\nSFO\n1630945\n425604\n26550493\n34.4008\n134.33\n\n\nSLC\n1403384\n205160\n10123371\n19.0164\n108.713\n\n\n\n\ntechnic note : understanding to use lambda funcion so that make a new columns.\n\npor_delay1 = (delay_info\n    .assign(\n        portion_delay = lambda x : \n        x.num_of_delays_total / x.num_of_flights_total,\n        average_min_delay = lambda x :\n        x.minutes_delayed_total  / x.num_of_delays_total / 60 )\n ### 2. Do you think the worst month to travel in summer? #### (Define worst months on each airport and show the chart of it) ###### people usually have their vacation in summer, so people might think summer have more delayed flight. However, the results shows that december is the worst month to travel because flight might delayed in december very often at any airport. Unexpectedly, June is the second proportion of number of delayed flights.\n\n\ntechnic note : understanding to order list of items and to replaces the axis with different format.\n\n.encode(\n        x =alt.X('month', sort = order_month, title = \"Month\"),\n        y = alt.Y('proportion_delayed', axis = alt.Axis(format= \"%\"), \n        title=\"portion_delayed\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nairport_code\nnum_of_delays_weather\nnum_of_delays_late_aircraft\n\n\n\n\nATL\n245.265\n1759.97\n\n\nDEN\n104.818\n1363.64\n\n\nIAD\n36.3182\n491.189\n\n\nORD\n157.311\n2043.83\n\n\nSAN\n32.7273\n534.856\n\n\nSFO\n78.6136\n940.682\n\n\nSLC\n51.75\n601.902\n\n\n\n\n\n\nairport_code\nlate_plus_weather\nrevise_late_arrive\n\n\n\n\nATL\n773.257\n1231.98\n\n\nDEN\n513.911\n954.551\n\n\nIAD\n183.675\n343.833\n\n\nORD\n770.459\n1430.68\n\n\nSAN\n193.184\n374.399\n\n\nSFO\n360.818\n658.477\n\n\nSLC\n232.32\n421.331\n\n\n\n\n\n\n\n\n\n\nmonth\nnum_of_delays_nas\nnum_of_delays_weather\n\n\n\n\nApril\n1216.4\n76.6104\n\n\nAugust\n1395.75\n111.935\n\n\nDecember\n1563.08\n131.632\n\n\nFebruary\n1408.7\n115.143\n\n\nJanuary\n1468.24\n126.859\n\n\nJuly\n1523.43\n137.195\n\n\nJune\n1610.32\n134.961\n\n\nMarch\n1373.14\n94.1579\n\n\nMay\n1353.77\n88.4156\n\n\nNovember\n1149.79\n63.6883\n\n\nOctober\n1322.21\n64.7051\n\n\nSeptember\n1134.66\n66.8052\n\n\n\n\n\n\nmonth\nrevise_nas\nnas_plus_weather\n\n\n\n\nApril\n486.561\n563.171\n\n\nAugust\n558.301\n670.236\n\n\nDecember\n1016\n1147.63\n\n\nFebruary\n915.656\n1030.8\n\n\nJanuary\n954.358\n1081.22\n\n\nJuly\n609.371\n746.566\n\n\nJune\n644.13\n779.091\n\n\nMarch\n892.544\n986.702\n\n\nMay\n541.506\n629.922\n\n\nNovember\n747.365\n811.053\n\n\nOctober\n859.433\n924.138\n\n\nSeptember\n737.531\n804.336\n\n\n\ntechnic note : understanding to use where function so that condition can be seted either True and False.\nlambda x :\n    np.where(\n        x.month.isin([\"April\",\"May\", \"June\", \"July\", \"August\"]), x.num_of_delays_nas * 0.4 ,\n        x.num_of_delays_nas * 0.65))\n\n\n\n\n\n\n\n\ntechnic note : understanding to arrage the data from the raw data and to make it group for analyzing.\npro_weather2 = (weather_total\n    .assign(pro_weather = lambda x : x. nas_plus_weather + x. late_plus_weather - x. num_of_delays_weather))\n\ngrouped = pro_weather2.groupby('airport_code').sum()\n\n\n\n\n\n\n\n     {\n            \"index\": 2,\n            \"airport_code\": \"IAD\",\n            \"airport_name\": \"\",\n            \"month\": \"January\",\n            \"year\": 2005.0,\n            \"num_of_flights_total\": 12381,\n            \"num_of_delays_carrier\": \"414\",\n            \"num_of_delays_late_aircraft\": 1058,\n            \"num_of_delays_nas\": 895,\n            \"num_of_delays_security\": 4,\n            \"num_of_delays_weather\": 61,\n            \"num_of_delays_total\": 2430,\n            \"minutes_delayed_carrier\": null,\n            \"minutes_delayed_late_aircraft\": 70919,\n            \"minutes_delayed_nas\": 35660.0,\n            \"minutes_delayed_security\": 208,\n            \"minutes_delayed_weather\": 4497,\n            \"minutes_delayed_total\": 134881\n        }\n\n\n\n\nimport pandas as pd\nimport altair as alt\nimport numpy as np\nimport json\n\nurl =\"https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json\"\n\nairport = pd.read_json(url)\n\n# %% missing data\nclean_data = airport\nclean_data.dropna()\nclean_data[\"month\"] = (airport\n    .month.replace('n/a', np.nan)\n    .fillna(method= 'ffill')\n    .replace('Febuary', 'February')\n)\n\nclean_data['num_of_delays_late_aircraft'] = (airport\n    .num_of_delays_late_aircraft\n    .replace(-999, airport.num_of_delays_late_aircraft.mean())\n)\n\nclean_data['num_of_flight_total'] = (airport\n    .num_of_flights_total.replace(np.nan, np.mean)\n)\n# %%\n''' Which airport has the worst delays? How did you choose to define “worst”? As part of your answer include a table that lists the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours, for each airport.'''\n\ndelay_info = clean_data.filter(['airport_code', 'month','num_of_flights_total','num_of_delays_total','minutes_delayed_total'])\n\n#%%\npor_delay = (delay_info\n    .assign(\n        portion_delay = lambda x : \n        x.num_of_delays_total / x.num_of_flights_total,\n        average_min_delay = lambda x :\n        x.minutes_delayed_total  / x.num_of_delays_total / 60 )\n    .groupby('airport_code').sum()\n    .reset_index()\n)\n\nprint(por_delay.to_markdown(index=False))\n#%%\npor_delay1 = (delay_info\n    .assign(\n        portion_delay = lambda x : \n        x.num_of_delays_total / x.num_of_flights_total,\n        average_min_delay = lambda x :\n        x.minutes_delayed_total  / x.num_of_delays_total / 60 )\n    .reset_index()\n)\n\nchart1 = (alt.Chart(por_delay1)\n    .mark_point()\n    .encode(\n        x =alt.X ('portion_delay'), \n        y = alt.Y('average_min_delay'), \n        color = 'airport_code')\n)\n\nchart1.save(\"chart1.png\")\n\n# %%\n'''What is the worst month to fly if you want to avoid delays? Include one chart to help support your answer, with the x-axis ordered by month. You also need to explain and justify how you chose to handle the missing Month data.'''\n#%%\norder_month= ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'Octorber','September', 'October', 'November', 'December']\n#%%\nworst_month = (clean_data\n    .assign(proportion_delayed = lambda x : x.\n    num_of_delays_total / x.num_of_flights_total)\n    .groupby('month').mean()\n    .reset_index()\n)\n\nchart2 = (alt.Chart(worst_month)\n    .mark_bar()\n    .encode(\n        x =alt.X('month', sort = order_month, title = \"Month\"),\n        y = alt.Y('proportion_delayed', axis = alt.Axis(format= \"%\"), \n        title=\"portion_delayed\"))\n    .properties(\n        width = 600, \n        title = \"Worst Month of delaying airplan\")\n)\n\nchart2.save(\"chart2.png\")\n# %%\n'''According to the BTS website the Weather category only accounts for severe weather delays. Other “mild” weather delays are included as part of the NAS category and the Late-Arriving Aircraft category. Calculate the total number of flights delayed by weather (either severe or mild) using these two rules:\n\n1.30% of all delayed flights in the Late-Arriving category are due to weather.\n2. From April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.'''\n\n#%%\nlate_info = (clean_data\n    .groupby('airport_code').mean()\n    .filter(['num_of_delays_weather','num_of_delays_late_aircraft'])\n    .reset_index()\n)\nprint(late_info.to_markdown(index=False))\n# %%\nthirty_late = (clean_data\n    .assign(late_plus_weather = lambda x : \n    x. num_of_delays_late_aircraft * 0.3 + x.num_of_delays_weather)\n    .assign(revise_late_arrive = lambda x : x. num_of_delays_late_aircraft * 0.7) \n    .groupby('airport_code').mean()\n    .filter(['late_plus_weather', 'revise_late_arrive'])\n    .reset_index()\n)\nprint(thirty_late.to_markdown(index=False))\n#%%\nlate_info2 = (clean_data\n    .groupby('month').mean()\n    .filter(['num_of_delays_nas','num_of_delays_weather'])\n    .reset_index()\n   \n)\n\nprint(late_info2.to_markdown(index=False))\n#%%\nforthy_late = (clean_data\n    .assign(revise_nas = lambda x :\n    np.where(\n        x.month.isin([\"April\",\"May\", \"June\", \"July\", \"August\"]), x.num_of_delays_nas * 0.4 ,\n        x.num_of_delays_nas * 0.65))\n    .assign(nas_plus_weather = lambda x :\n        x. revise_nas + x.num_of_delays_weather )\n    .groupby('month').mean()\n    .filter(['revise_nas', 'nas_plus_weather'])\n    .reset_index()\n)\nprint(forthy_late.to_markdown(index=False))\n\n# %%\n'''Create a barplot showing the proportion of all flights that are delayed by weather at each airport. What do you learn from this graph (Careful to handle the missing Late Aircraft data correctly)?'''\n# %%\nweather_total = (clean_data\n    .assign(revise_nas = lambda x :\n    np.where(\n        x.month.isin([\"April\",\"May\", \"June\", \"July\", \"August\"]), x.num_of_delays_nas * 0.4 ,\n        x.num_of_delays_nas * 0.65))\n    .assign(nas_plus_weather = lambda x :\n        x. revise_nas + x.num_of_delays_weather )\n    .assign(late_plus_weather = lambda x : \n    x. num_of_delays_late_aircraft * 0.3 + x.num_of_delays_weather)\n    .assign(revise_late_arrive = lambda x : x. num_of_delays_late_aircraft * 0.7) \n    #.groupby('airport_code').mean()\n    #.filter(['nas_plus_weather', 'late_plus_weather'])\n    #.reset_index()\n)\n#%%\npro_weather2 = (weather_total\n    .assign(pro_weather = lambda x : x. nas_plus_weather + x. late_plus_weather - x. num_of_delays_weather))\n\ngrouped = pro_weather2.groupby('airport_code').sum()\n#%%   \ngrouped['pro_weather_delayed'] = grouped.pro_weather/ grouped.num_of_flights_total \ngrouped = grouped.reset_index()\n# %%\nchart4 = (alt.Chart(grouped)\n    .mark_bar()\n    .encode(\n        x =alt.X ('airport_code', title = \"air_code\"),\n        y = alt.Y('pro_weather_delayed', axis = alt.Axis(format= \"%\"), \n        title=\"portion_delayed_weather\" ))\n        .properties(width = 600, title = \"weatehr delayed airplan\")\n)\nchart4.save(\"chart4.png\")\n# %%\n'''Fix all of the varied NA types in the data to be consistent and save the file back out in the same format that was provided (this file shouldn’t have the missing values replaced with a value). Include one record example from your exported JSON file that has a missing value (No imputation in this file).'''\n#%%\njson_data= airport.to_json(orient='table')\nloading = json.loads(json_data)\nprint(json.dumps(loading, indent = 4))\n\n''' paste JS \n     {\n            \"index\": 2,\n            \"airport_code\": \"IAD\",\n            \"airport_name\": \"\",\n            \"month\": \"January\",\n            \"year\": 2005.0,\n            \"num_of_flights_total\": 12381,\n            \"num_of_delays_carrier\": \"414\",\n            \"num_of_delays_late_aircraft\": 1058,\n            \"num_of_delays_nas\": 895,\n            \"num_of_delays_security\": 4,\n            \"num_of_delays_weather\": 61,\n            \"num_of_delays_total\": 2430,\n            \"minutes_delayed_carrier\": null,\n            \"minutes_delayed_late_aircraft\": 70919,\n            \"minutes_delayed_nas\": 35660.0,\n            \"minutes_delayed_security\": 208,\n            \"minutes_delayed_weather\": 4497,\n            \"minutes_delayed_total\": 134881\n        }\n'''\ncommented python code from .py file"
  },
  {
    "objectID": "data_analysis/ds250/project2.html",
    "href": "data_analysis/ds250/project2.html",
    "title": "Analyzing Flight Delays: Causes and Timining Analysis",
    "section": "",
    "text": "When using airport, it seems natual to wait airplan for a while without knowing the reason. People try to have more time spaces between flights because of late flight. I can figure out why the flight is late so often using the date. I found the raw and not defined date in the frame and filled out variables so that there have no missing values. when it comes to analyze the data, I reorganize data into right columns so that I expect the reason of late flight and the time of late flight with understading.\n\n\n\n\nWhat is the worst delays? It is depends on the frequency and how much times you wait. It is possible to have late flight, but it is not many hours. On the opposition, it is possible to wait without reasons, but it can be very rare. For adding two values, I would prefer to use SLC or SAN because it shows average of delaying time is less than others and the frequency of delayed flight is less than others. The SFO airport or ORD have high average delayed minites and high proportion of delayed flight.\n\n\n\n\n\n\n\n\n\n\n\n\nairport_code\nnum_of_flights_total\nnum_of_delays_total\nminutes_delayed_total\nportion_delay\naverage_min_delay\n\n\n\n\nATL\n4430047\n902443\n53983926\n26.6177\n130.614\n\n\nDEN\n2513974\n468519\n25173381\n24.5232\n116.501\n\n\nIAD\n851571\n168467\n10283478\n25.7612\n132.428\n\n\nORD\n3597588\n830825\n56356129\n30.2032\n145.955\n\n\nSAN\n917862\n175132\n8276248\n25.0415\n103.492\n\n\nSFO\n1630945\n425604\n26550493\n34.4008\n134.33\n\n\nSLC\n1403384\n205160\n10123371\n19.0164\n108.713\n\n\n\n\ntechnic note : understanding to use lambda funcion so that make a new columns.\n\npor_delay1 = (delay_info\n    .assign(\n        portion_delay = lambda x : \n        x.num_of_delays_total / x.num_of_flights_total,\n        average_min_delay = lambda x :\n        x.minutes_delayed_total  / x.num_of_delays_total / 60 )\n\n\n\n\n\n\npeople usually have their vacation in summer, so people might think summer have more delayed flight. However, the results shows that december is the worst month to travel because flight might delayed in december very often at any airport. Unexpectedly, June is the second proportion of number of delayed flights.\n\n\ntechnic note : understanding to order list of items and to replaces the axis with different format.\n\n.encode(\n        x =alt.X('month', sort = order_month, title = \"Month\"),\n        y = alt.Y('proportion_delayed', axis = alt.Axis(format= \"%\"), \n        title=\"portion_delayed\"))\n\n\n\n\nThe tables shows that the average of number of delayed flight increase when mild weather includes to severe weather.\n\n\n\n\n\n\n\n\n\n\nairport_code\nnum_of_delays_weather\nnum_of_delays_late_aircraft\n\n\n\n\nATL\n245.265\n1759.97\n\n\nDEN\n104.818\n1363.64\n\n\nIAD\n36.3182\n491.189\n\n\nORD\n157.311\n2043.83\n\n\nSAN\n32.7273\n534.856\n\n\nSFO\n78.6136\n940.682\n\n\nSLC\n51.75\n601.902\n\n\n\n\n\n\nairport_code\nlate_plus_weather\nrevise_late_arrive\n\n\n\n\nATL\n773.257\n1231.98\n\n\nDEN\n513.911\n954.551\n\n\nIAD\n183.675\n343.833\n\n\nORD\n770.459\n1430.68\n\n\nSAN\n193.184\n374.399\n\n\nSFO\n360.818\n658.477\n\n\nSLC\n232.32\n421.331\n\n\n\n\n\n\n\n\n\nmonth\nnum_of_delays_nas\nnum_of_delays_weather\n\n\n\n\nApril\n1216.4\n76.6104\n\n\nAugust\n1395.75\n111.935\n\n\nDecember\n1563.08\n131.632\n\n\nFebruary\n1408.7\n115.143\n\n\nJanuary\n1468.24\n126.859\n\n\nJuly\n1523.43\n137.195\n\n\nJune\n1610.32\n134.961\n\n\nMarch\n1373.14\n94.1579\n\n\nMay\n1353.77\n88.4156\n\n\nNovember\n1149.79\n63.6883\n\n\nOctober\n1322.21\n64.7051\n\n\nSeptember\n1134.66\n66.8052\n\n\n\n\n\n\nmonth\nrevise_nas\nnas_plus_weather\n\n\n\n\nApril\n486.561\n563.171\n\n\nAugust\n558.301\n670.236\n\n\nDecember\n1016\n1147.63\n\n\nFebruary\n915.656\n1030.8\n\n\nJanuary\n954.358\n1081.22\n\n\nJuly\n609.371\n746.566\n\n\nJune\n644.13\n779.091\n\n\nMarch\n892.544\n986.702\n\n\nMay\n541.506\n629.922\n\n\nNovember\n747.365\n811.053\n\n\nOctober\n859.433\n924.138\n\n\nSeptember\n737.531\n804.336\n\n\n\ntechnic note : understanding to use where function so that condition can be seted either True and False.\nlambda x :\n    np.where(\n        x.month.isin([\"April\",\"May\", \"June\", \"July\", \"August\"]), x.num_of_delays_nas * 0.4 ,\n        x.num_of_delays_nas * 0.65))\n\n\n\n\n\n\nPeople start to think what is the problem to wait so long when they still can not get any information from the airport. People can assumed, especially SFO, weather can be the reason of delayed flight because the chart shows almost 10% of delayed flight from total number of flights which is high level of it.\n\ntechnic note : understanding to arrage the data from the raw data and to make it group for analyzing.\npro_weather2 = (weather_total\n    .assign(pro_weather = lambda x : x. nas_plus_weather + x. late_plus_weather - x. num_of_delays_weather))\n\ngrouped = pro_weather2.groupby('airport_code').sum()\n\n\n\n\n\n\n     {\n            \"index\": 2,\n            \"airport_code\": \"IAD\",\n            \"airport_name\": \"\",\n            \"month\": \"January\",\n            \"year\": 2005.0,\n            \"num_of_flights_total\": 12381,\n            \"num_of_delays_carrier\": \"414\",\n            \"num_of_delays_late_aircraft\": 1058,\n            \"num_of_delays_nas\": 895,\n            \"num_of_delays_security\": 4,\n            \"num_of_delays_weather\": 61,\n            \"num_of_delays_total\": 2430,\n            \"minutes_delayed_carrier\": null,\n            \"minutes_delayed_late_aircraft\": 70919,\n            \"minutes_delayed_nas\": 35660.0,\n            \"minutes_delayed_security\": 208,\n            \"minutes_delayed_weather\": 4497,\n            \"minutes_delayed_total\": 134881\n        }\n\n\n\n\nimport pandas as pd\nimport altair as alt\nimport numpy as np\nimport json\n\nurl =\"https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json\"\n\nairport = pd.read_json(url)\n\n# %% missing data\nclean_data = airport\nclean_data.dropna()\nclean_data[\"month\"] = (airport\n    .month.replace('n/a', np.nan)\n    .fillna(method= 'ffill')\n    .replace('Febuary', 'February')\n)\n\nclean_data['num_of_delays_late_aircraft'] = (airport\n    .num_of_delays_late_aircraft\n    .replace(-999, airport.num_of_delays_late_aircraft.mean())\n)\n\nclean_data['num_of_flight_total'] = (airport\n    .num_of_flights_total.replace(np.nan, np.mean)\n)\n# %%\n''' Which airport has the worst delays? How did you choose to define “worst”? As part of your answer include a table that lists the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours, for each airport.'''\n\ndelay_info = clean_data.filter(['airport_code', 'month','num_of_flights_total','num_of_delays_total','minutes_delayed_total'])\n\n#%%\npor_delay = (delay_info\n    .assign(\n        portion_delay = lambda x : \n        x.num_of_delays_total / x.num_of_flights_total,\n        average_min_delay = lambda x :\n        x.minutes_delayed_total  / x.num_of_delays_total / 60 )\n    .groupby('airport_code').sum()\n    .reset_index()\n)\n\nprint(por_delay.to_markdown(index=False))\n#%%\npor_delay1 = (delay_info\n    .assign(\n        portion_delay = lambda x : \n        x.num_of_delays_total / x.num_of_flights_total,\n        average_min_delay = lambda x :\n        x.minutes_delayed_total  / x.num_of_delays_total / 60 )\n    .reset_index()\n)\n\nchart1 = (alt.Chart(por_delay1)\n    .mark_point()\n    .encode(\n        x =alt.X ('portion_delay'), \n        y = alt.Y('average_min_delay'), \n        color = 'airport_code')\n)\n\nchart1.save(\"chart1.png\")\n\n# %%\n'''What is the worst month to fly if you want to avoid delays? Include one chart to help support your answer, with the x-axis ordered by month. You also need to explain and justify how you chose to handle the missing Month data.'''\n#%%\norder_month= ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'Octorber','September', 'October', 'November', 'December']\n#%%\nworst_month = (clean_data\n    .assign(proportion_delayed = lambda x : x.\n    num_of_delays_total / x.num_of_flights_total)\n    .groupby('month').mean()\n    .reset_index()\n)\n\nchart2 = (alt.Chart(worst_month)\n    .mark_bar()\n    .encode(\n        x =alt.X('month', sort = order_month, title = \"Month\"),\n        y = alt.Y('proportion_delayed', axis = alt.Axis(format= \"%\"), \n        title=\"portion_delayed\"))\n    .properties(\n        width = 600, \n        title = \"Worst Month of delaying airplan\")\n)\n\nchart2.save(\"chart2.png\")\n# %%\n'''According to the BTS website the Weather category only accounts for severe weather delays. Other “mild” weather delays are included as part of the NAS category and the Late-Arriving Aircraft category. Calculate the total number of flights delayed by weather (either severe or mild) using these two rules:\n\n1.30% of all delayed flights in the Late-Arriving category are due to weather.\n2. From April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.'''\n\n#%%\nlate_info = (clean_data\n    .groupby('airport_code').mean()\n    .filter(['num_of_delays_weather','num_of_delays_late_aircraft'])\n    .reset_index()\n)\nprint(late_info.to_markdown(index=False))\n# %%\nthirty_late = (clean_data\n    .assign(late_plus_weather = lambda x : \n    x. num_of_delays_late_aircraft * 0.3 + x.num_of_delays_weather)\n    .assign(revise_late_arrive = lambda x : x. num_of_delays_late_aircraft * 0.7) \n    .groupby('airport_code').mean()\n    .filter(['late_plus_weather', 'revise_late_arrive'])\n    .reset_index()\n)\nprint(thirty_late.to_markdown(index=False))\n#%%\nlate_info2 = (clean_data\n    .groupby('month').mean()\n    .filter(['num_of_delays_nas','num_of_delays_weather'])\n    .reset_index()\n   \n)\n\nprint(late_info2.to_markdown(index=False))\n#%%\nforthy_late = (clean_data\n    .assign(revise_nas = lambda x :\n    np.where(\n        x.month.isin([\"April\",\"May\", \"June\", \"July\", \"August\"]), x.num_of_delays_nas * 0.4 ,\n        x.num_of_delays_nas * 0.65))\n    .assign(nas_plus_weather = lambda x :\n        x. revise_nas + x.num_of_delays_weather )\n    .groupby('month').mean()\n    .filter(['revise_nas', 'nas_plus_weather'])\n    .reset_index()\n)\nprint(forthy_late.to_markdown(index=False))\n\n# %%\n'''Create a barplot showing the proportion of all flights that are delayed by weather at each airport. What do you learn from this graph (Careful to handle the missing Late Aircraft data correctly)?'''\n# %%\nweather_total = (clean_data\n    .assign(revise_nas = lambda x :\n    np.where(\n        x.month.isin([\"April\",\"May\", \"June\", \"July\", \"August\"]), x.num_of_delays_nas * 0.4 ,\n        x.num_of_delays_nas * 0.65))\n    .assign(nas_plus_weather = lambda x :\n        x. revise_nas + x.num_of_delays_weather )\n    .assign(late_plus_weather = lambda x : \n    x. num_of_delays_late_aircraft * 0.3 + x.num_of_delays_weather)\n    .assign(revise_late_arrive = lambda x : x. num_of_delays_late_aircraft * 0.7) \n    #.groupby('airport_code').mean()\n    #.filter(['nas_plus_weather', 'late_plus_weather'])\n    #.reset_index()\n)\n#%%\npro_weather2 = (weather_total\n    .assign(pro_weather = lambda x : x. nas_plus_weather + x. late_plus_weather - x. num_of_delays_weather))\n\ngrouped = pro_weather2.groupby('airport_code').sum()\n#%%   \ngrouped['pro_weather_delayed'] = grouped.pro_weather/ grouped.num_of_flights_total \ngrouped = grouped.reset_index()\n# %%\nchart4 = (alt.Chart(grouped)\n    .mark_bar()\n    .encode(\n        x =alt.X ('airport_code', title = \"air_code\"),\n        y = alt.Y('pro_weather_delayed', axis = alt.Axis(format= \"%\"), \n        title=\"portion_delayed_weather\" ))\n        .properties(width = 600, title = \"weatehr delayed airplan\")\n)\nchart4.save(\"chart4.png\")\n# %%\n'''Fix all of the varied NA types in the data to be consistent and save the file back out in the same format that was provided (this file shouldn’t have the missing values replaced with a value). Include one record example from your exported JSON file that has a missing value (No imputation in this file).'''\n#%%\njson_data= airport.to_json(orient='table')\nloading = json.loads(json_data)\nprint(json.dumps(loading, indent = 4))\n\n''' paste JS \n     {\n            \"index\": 2,\n            \"airport_code\": \"IAD\",\n            \"airport_name\": \"\",\n            \"month\": \"January\",\n            \"year\": 2005.0,\n            \"num_of_flights_total\": 12381,\n            \"num_of_delays_carrier\": \"414\",\n            \"num_of_delays_late_aircraft\": 1058,\n            \"num_of_delays_nas\": 895,\n            \"num_of_delays_security\": 4,\n            \"num_of_delays_weather\": 61,\n            \"num_of_delays_total\": 2430,\n            \"minutes_delayed_carrier\": null,\n            \"minutes_delayed_late_aircraft\": 70919,\n            \"minutes_delayed_nas\": 35660.0,\n            \"minutes_delayed_security\": 208,\n            \"minutes_delayed_weather\": 4497,\n            \"minutes_delayed_total\": 134881\n        }\n'''\ncommented python code from .py file"
  },
  {
    "objectID": "data_analysis/ds250/project2.html#elevator-pitch",
    "href": "data_analysis/ds250/project2.html#elevator-pitch",
    "title": "Analyzing Flight Delays: Causes and Timining Analysis",
    "section": "",
    "text": "When using airport, it seems natual to wait airplan for a while without knowing the reason. People try to have more time spaces between flights because of late flight. I can figure out why the flight is late so often using the date. I found the raw and not defined date in the frame and filled out variables so that there have no missing values. when it comes to analyze the data, I reorganize data into right columns so that I expect the reason of late flight and the time of late flight with understading.\n\n\n\n\nWhat is the worst delays? It is depends on the frequency and how much times you wait. It is possible to have late flight, but it is not many hours. On the opposition, it is possible to wait without reasons, but it can be very rare. For adding two values, I would prefer to use SLC or SAN because it shows average of delaying time is less than others and the frequency of delayed flight is less than others. The SFO airport or ORD have high average delayed minites and high proportion of delayed flight.\n\n\n\n\n\n\n\n\n\n\n\n\nairport_code\nnum_of_flights_total\nnum_of_delays_total\nminutes_delayed_total\nportion_delay\naverage_min_delay\n\n\n\n\nATL\n4430047\n902443\n53983926\n26.6177\n130.614\n\n\nDEN\n2513974\n468519\n25173381\n24.5232\n116.501\n\n\nIAD\n851571\n168467\n10283478\n25.7612\n132.428\n\n\nORD\n3597588\n830825\n56356129\n30.2032\n145.955\n\n\nSAN\n917862\n175132\n8276248\n25.0415\n103.492\n\n\nSFO\n1630945\n425604\n26550493\n34.4008\n134.33\n\n\nSLC\n1403384\n205160\n10123371\n19.0164\n108.713\n\n\n\n\ntechnic note : understanding to use lambda funcion so that make a new columns.\n\npor_delay1 = (delay_info\n    .assign(\n        portion_delay = lambda x : \n        x.num_of_delays_total / x.num_of_flights_total,\n        average_min_delay = lambda x :\n        x.minutes_delayed_total  / x.num_of_delays_total / 60 )\n\n\n\n\n\n\npeople usually have their vacation in summer, so people might think summer have more delayed flight. However, the results shows that december is the worst month to travel because flight might delayed in december very often at any airport. Unexpectedly, June is the second proportion of number of delayed flights.\n\n\ntechnic note : understanding to order list of items and to replaces the axis with different format.\n\n.encode(\n        x =alt.X('month', sort = order_month, title = \"Month\"),\n        y = alt.Y('proportion_delayed', axis = alt.Axis(format= \"%\"), \n        title=\"portion_delayed\"))\n\n\n\n\nThe tables shows that the average of number of delayed flight increase when mild weather includes to severe weather.\n\n\n\n\n\n\n\n\n\n\nairport_code\nnum_of_delays_weather\nnum_of_delays_late_aircraft\n\n\n\n\nATL\n245.265\n1759.97\n\n\nDEN\n104.818\n1363.64\n\n\nIAD\n36.3182\n491.189\n\n\nORD\n157.311\n2043.83\n\n\nSAN\n32.7273\n534.856\n\n\nSFO\n78.6136\n940.682\n\n\nSLC\n51.75\n601.902\n\n\n\n\n\n\nairport_code\nlate_plus_weather\nrevise_late_arrive\n\n\n\n\nATL\n773.257\n1231.98\n\n\nDEN\n513.911\n954.551\n\n\nIAD\n183.675\n343.833\n\n\nORD\n770.459\n1430.68\n\n\nSAN\n193.184\n374.399\n\n\nSFO\n360.818\n658.477\n\n\nSLC\n232.32\n421.331\n\n\n\n\n\n\n\n\n\nmonth\nnum_of_delays_nas\nnum_of_delays_weather\n\n\n\n\nApril\n1216.4\n76.6104\n\n\nAugust\n1395.75\n111.935\n\n\nDecember\n1563.08\n131.632\n\n\nFebruary\n1408.7\n115.143\n\n\nJanuary\n1468.24\n126.859\n\n\nJuly\n1523.43\n137.195\n\n\nJune\n1610.32\n134.961\n\n\nMarch\n1373.14\n94.1579\n\n\nMay\n1353.77\n88.4156\n\n\nNovember\n1149.79\n63.6883\n\n\nOctober\n1322.21\n64.7051\n\n\nSeptember\n1134.66\n66.8052\n\n\n\n\n\n\nmonth\nrevise_nas\nnas_plus_weather\n\n\n\n\nApril\n486.561\n563.171\n\n\nAugust\n558.301\n670.236\n\n\nDecember\n1016\n1147.63\n\n\nFebruary\n915.656\n1030.8\n\n\nJanuary\n954.358\n1081.22\n\n\nJuly\n609.371\n746.566\n\n\nJune\n644.13\n779.091\n\n\nMarch\n892.544\n986.702\n\n\nMay\n541.506\n629.922\n\n\nNovember\n747.365\n811.053\n\n\nOctober\n859.433\n924.138\n\n\nSeptember\n737.531\n804.336\n\n\n\ntechnic note : understanding to use where function so that condition can be seted either True and False.\nlambda x :\n    np.where(\n        x.month.isin([\"April\",\"May\", \"June\", \"July\", \"August\"]), x.num_of_delays_nas * 0.4 ,\n        x.num_of_delays_nas * 0.65))\n\n\n\n\n\n\nPeople start to think what is the problem to wait so long when they still can not get any information from the airport. People can assumed, especially SFO, weather can be the reason of delayed flight because the chart shows almost 10% of delayed flight from total number of flights which is high level of it.\n\ntechnic note : understanding to arrage the data from the raw data and to make it group for analyzing.\npro_weather2 = (weather_total\n    .assign(pro_weather = lambda x : x. nas_plus_weather + x. late_plus_weather - x. num_of_delays_weather))\n\ngrouped = pro_weather2.groupby('airport_code').sum()\n\n\n\n\n\n\n     {\n            \"index\": 2,\n            \"airport_code\": \"IAD\",\n            \"airport_name\": \"\",\n            \"month\": \"January\",\n            \"year\": 2005.0,\n            \"num_of_flights_total\": 12381,\n            \"num_of_delays_carrier\": \"414\",\n            \"num_of_delays_late_aircraft\": 1058,\n            \"num_of_delays_nas\": 895,\n            \"num_of_delays_security\": 4,\n            \"num_of_delays_weather\": 61,\n            \"num_of_delays_total\": 2430,\n            \"minutes_delayed_carrier\": null,\n            \"minutes_delayed_late_aircraft\": 70919,\n            \"minutes_delayed_nas\": 35660.0,\n            \"minutes_delayed_security\": 208,\n            \"minutes_delayed_weather\": 4497,\n            \"minutes_delayed_total\": 134881\n        }\n\n\n\n\nimport pandas as pd\nimport altair as alt\nimport numpy as np\nimport json\n\nurl =\"https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json\"\n\nairport = pd.read_json(url)\n\n# %% missing data\nclean_data = airport\nclean_data.dropna()\nclean_data[\"month\"] = (airport\n    .month.replace('n/a', np.nan)\n    .fillna(method= 'ffill')\n    .replace('Febuary', 'February')\n)\n\nclean_data['num_of_delays_late_aircraft'] = (airport\n    .num_of_delays_late_aircraft\n    .replace(-999, airport.num_of_delays_late_aircraft.mean())\n)\n\nclean_data['num_of_flight_total'] = (airport\n    .num_of_flights_total.replace(np.nan, np.mean)\n)\n# %%\n''' Which airport has the worst delays? How did you choose to define “worst”? As part of your answer include a table that lists the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours, for each airport.'''\n\ndelay_info = clean_data.filter(['airport_code', 'month','num_of_flights_total','num_of_delays_total','minutes_delayed_total'])\n\n#%%\npor_delay = (delay_info\n    .assign(\n        portion_delay = lambda x : \n        x.num_of_delays_total / x.num_of_flights_total,\n        average_min_delay = lambda x :\n        x.minutes_delayed_total  / x.num_of_delays_total / 60 )\n    .groupby('airport_code').sum()\n    .reset_index()\n)\n\nprint(por_delay.to_markdown(index=False))\n#%%\npor_delay1 = (delay_info\n    .assign(\n        portion_delay = lambda x : \n        x.num_of_delays_total / x.num_of_flights_total,\n        average_min_delay = lambda x :\n        x.minutes_delayed_total  / x.num_of_delays_total / 60 )\n    .reset_index()\n)\n\nchart1 = (alt.Chart(por_delay1)\n    .mark_point()\n    .encode(\n        x =alt.X ('portion_delay'), \n        y = alt.Y('average_min_delay'), \n        color = 'airport_code')\n)\n\nchart1.save(\"chart1.png\")\n\n# %%\n'''What is the worst month to fly if you want to avoid delays? Include one chart to help support your answer, with the x-axis ordered by month. You also need to explain and justify how you chose to handle the missing Month data.'''\n#%%\norder_month= ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'Octorber','September', 'October', 'November', 'December']\n#%%\nworst_month = (clean_data\n    .assign(proportion_delayed = lambda x : x.\n    num_of_delays_total / x.num_of_flights_total)\n    .groupby('month').mean()\n    .reset_index()\n)\n\nchart2 = (alt.Chart(worst_month)\n    .mark_bar()\n    .encode(\n        x =alt.X('month', sort = order_month, title = \"Month\"),\n        y = alt.Y('proportion_delayed', axis = alt.Axis(format= \"%\"), \n        title=\"portion_delayed\"))\n    .properties(\n        width = 600, \n        title = \"Worst Month of delaying airplan\")\n)\n\nchart2.save(\"chart2.png\")\n# %%\n'''According to the BTS website the Weather category only accounts for severe weather delays. Other “mild” weather delays are included as part of the NAS category and the Late-Arriving Aircraft category. Calculate the total number of flights delayed by weather (either severe or mild) using these two rules:\n\n1.30% of all delayed flights in the Late-Arriving category are due to weather.\n2. From April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.'''\n\n#%%\nlate_info = (clean_data\n    .groupby('airport_code').mean()\n    .filter(['num_of_delays_weather','num_of_delays_late_aircraft'])\n    .reset_index()\n)\nprint(late_info.to_markdown(index=False))\n# %%\nthirty_late = (clean_data\n    .assign(late_plus_weather = lambda x : \n    x. num_of_delays_late_aircraft * 0.3 + x.num_of_delays_weather)\n    .assign(revise_late_arrive = lambda x : x. num_of_delays_late_aircraft * 0.7) \n    .groupby('airport_code').mean()\n    .filter(['late_plus_weather', 'revise_late_arrive'])\n    .reset_index()\n)\nprint(thirty_late.to_markdown(index=False))\n#%%\nlate_info2 = (clean_data\n    .groupby('month').mean()\n    .filter(['num_of_delays_nas','num_of_delays_weather'])\n    .reset_index()\n   \n)\n\nprint(late_info2.to_markdown(index=False))\n#%%\nforthy_late = (clean_data\n    .assign(revise_nas = lambda x :\n    np.where(\n        x.month.isin([\"April\",\"May\", \"June\", \"July\", \"August\"]), x.num_of_delays_nas * 0.4 ,\n        x.num_of_delays_nas * 0.65))\n    .assign(nas_plus_weather = lambda x :\n        x. revise_nas + x.num_of_delays_weather )\n    .groupby('month').mean()\n    .filter(['revise_nas', 'nas_plus_weather'])\n    .reset_index()\n)\nprint(forthy_late.to_markdown(index=False))\n\n# %%\n'''Create a barplot showing the proportion of all flights that are delayed by weather at each airport. What do you learn from this graph (Careful to handle the missing Late Aircraft data correctly)?'''\n# %%\nweather_total = (clean_data\n    .assign(revise_nas = lambda x :\n    np.where(\n        x.month.isin([\"April\",\"May\", \"June\", \"July\", \"August\"]), x.num_of_delays_nas * 0.4 ,\n        x.num_of_delays_nas * 0.65))\n    .assign(nas_plus_weather = lambda x :\n        x. revise_nas + x.num_of_delays_weather )\n    .assign(late_plus_weather = lambda x : \n    x. num_of_delays_late_aircraft * 0.3 + x.num_of_delays_weather)\n    .assign(revise_late_arrive = lambda x : x. num_of_delays_late_aircraft * 0.7) \n    #.groupby('airport_code').mean()\n    #.filter(['nas_plus_weather', 'late_plus_weather'])\n    #.reset_index()\n)\n#%%\npro_weather2 = (weather_total\n    .assign(pro_weather = lambda x : x. nas_plus_weather + x. late_plus_weather - x. num_of_delays_weather))\n\ngrouped = pro_weather2.groupby('airport_code').sum()\n#%%   \ngrouped['pro_weather_delayed'] = grouped.pro_weather/ grouped.num_of_flights_total \ngrouped = grouped.reset_index()\n# %%\nchart4 = (alt.Chart(grouped)\n    .mark_bar()\n    .encode(\n        x =alt.X ('airport_code', title = \"air_code\"),\n        y = alt.Y('pro_weather_delayed', axis = alt.Axis(format= \"%\"), \n        title=\"portion_delayed_weather\" ))\n        .properties(width = 600, title = \"weatehr delayed airplan\")\n)\nchart4.save(\"chart4.png\")\n# %%\n'''Fix all of the varied NA types in the data to be consistent and save the file back out in the same format that was provided (this file shouldn’t have the missing values replaced with a value). Include one record example from your exported JSON file that has a missing value (No imputation in this file).'''\n#%%\njson_data= airport.to_json(orient='table')\nloading = json.loads(json_data)\nprint(json.dumps(loading, indent = 4))\n\n''' paste JS \n     {\n            \"index\": 2,\n            \"airport_code\": \"IAD\",\n            \"airport_name\": \"\",\n            \"month\": \"January\",\n            \"year\": 2005.0,\n            \"num_of_flights_total\": 12381,\n            \"num_of_delays_carrier\": \"414\",\n            \"num_of_delays_late_aircraft\": 1058,\n            \"num_of_delays_nas\": 895,\n            \"num_of_delays_security\": 4,\n            \"num_of_delays_weather\": 61,\n            \"num_of_delays_total\": 2430,\n            \"minutes_delayed_carrier\": null,\n            \"minutes_delayed_late_aircraft\": 70919,\n            \"minutes_delayed_nas\": 35660.0,\n            \"minutes_delayed_security\": 208,\n            \"minutes_delayed_weather\": 4497,\n            \"minutes_delayed_total\": 134881\n        }\n'''\ncommented python code from .py file"
  },
  {
    "objectID": "data_analysis/ds250/ds250.html",
    "href": "data_analysis/ds250/ds250.html",
    "title": "Gakyeong Bae",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "visualization/ba315/ba315_project.html",
    "href": "visualization/ba315/ba315_project.html",
    "title": "Fund Recommendation(Power BI)",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_analysis/math425/House_analysis.html",
    "href": "data_analysis/math425/House_analysis.html",
    "title": "Housing Selling Prices",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_analysis/math425/House_analysis.html#Elavorate Pitch",
    "href": "data_analysis/math425/House_analysis.html#Elavorate Pitch",
    "title": "My Portfolio",
    "section": "",
    "text": "In summary, sellers tend to achieve higher selling prices when their property features over 7 combined rooms and toilets, or when the overall Quality impression is superior. This connection between higher selling prices and these features becomes more apparent when the property has a larger total square footage, highlighting the influence of both the number of rooms/toilets and quality on the sale price."
  },
  {
    "objectID": "data_analysis/math425/House_analysis.html#data",
    "href": "data_analysis/math425/House_analysis.html#data",
    "title": "Housing Selling Price",
    "section": "\nData\n",
    "text": "Data"
  },
  {
    "objectID": "data_analysis/math425/House_analysis.html#mutate",
    "href": "data_analysis/math425/House_analysis.html#mutate",
    "title": "Housing Selling Price",
    "section": "\nMutate\n",
    "text": "Mutate\n\nhouses &lt;- houses %&gt;%\n  mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF) %&gt;% \n  mutate(TotalSF = ifelse(TotalSF &gt; 7000, 2000, TotalSF))\n\nhouses &lt;- houses %&gt;%\n  mutate(TotalBT = FullBath + HalfBath )\n\nhouses &lt;- houses %&gt;%\n  mutate(totalN = TotalBT + TotRmsAbvGrd) %&gt;%\n  mutate(totalN = case_when(\n         totalN &lt;7 ~ \"small\",\n         totalN &lt;11 ~ \"mid\",\n         totalN &lt;15 ~ \"large\",\n         totalN &lt; 17 ~\"Extra\"),\n         totalN = as.factor(totalN))\n\n# overallqual\nhouses &lt;- houses %&gt;%\n  mutate(OverallQual = as.factor(OverallQual),\n         OverallQual =\n           case_when(OverallQual %in% c(1, 2, 3) ~ \"low\",\n                     OverallQual %in% c(4, 5, 6, 7) ~ \"mid\",\n                     OverallQual %in% c(8, 9, 10) ~ \"high\"),\n         OverallQual = as.factor(OverallQual)\n\n           ) \n\nmylm &lt;- lm(SalePrice~ totalN + TotalSF + OverallQual + totalN:TotalSF +OverallQual:TotalSF \n           , data=houses)\nsummary(mylm)\n## \n## Call:\n## lm(formula = SalePrice ~ totalN + TotalSF + OverallQual + totalN:TotalSF + \n##     OverallQual:TotalSF, data = houses)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -180788  -18532     759   20433  187531 \n## \n## Coefficients:\n##                          Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)            -1.142e+05  4.352e+04  -2.625 0.008752 ** \n## totalNlarge             7.876e+04  4.549e+04   1.731 0.083614 .  \n## totalNmid               6.655e+04  4.619e+04   1.441 0.149920    \n## totalNsmall             6.861e+04  4.735e+04   1.449 0.147519    \n## TotalSF                 1.212e+02  1.068e+01  11.347  &lt; 2e-16 ***\n## OverallQuallow          9.294e+04  2.605e+04   3.568 0.000371 ***\n## OverallQualmid          7.795e+04  1.526e+04   5.108 3.68e-07 ***\n## totalNlarge:TotalSF    -2.332e+01  1.126e+01  -2.071 0.038517 *  \n## totalNmid:TotalSF      -2.202e+01  1.156e+01  -1.905 0.057034 .  \n## totalNsmall:TotalSF    -2.696e+01  1.279e+01  -2.108 0.035234 *  \n## TotalSF:OverallQuallow -7.512e+01  1.361e+01  -5.521 3.98e-08 ***\n## TotalSF:OverallQualmid -4.512e+01  4.593e+00  -9.824  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 36290 on 1448 degrees of freedom\n## Multiple R-squared:  0.7929, Adjusted R-squared:  0.7913 \n## F-statistic:   504 on 11 and 1448 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "data_analysis/math425/House_analysis.html#validation",
    "href": "data_analysis/math425/House_analysis.html#validation",
    "title": "Housing Selling Price",
    "section": "\nvalidation\n",
    "text": "validation\n\nset.seed(115)\n\nnum_rows &lt;- 700 #1460 total\nkeep &lt;- sample(1:nrow(houses), num_rows)\n\nval &lt;- houses[keep, ] #Use this in the lm(..., data=mytrain) it is like \"rbdata\"\n\nval2 &lt;- houses[-keep, ] #Use this in the predict(..., newdata=mytest) it is like \"rbdata2\"\n\n\nval.lm &lt;- lm(SalePrice~ totalN + TotalSF + OverallQual + totalN:TotalSF +OverallQual:TotalSF \n           , data=val)\n#summary(val.lm)\n\nval2.lm &lt;-lm(SalePrice~ totalN + TotalSF + OverallQual + totalN:TotalSF +OverallQual:TotalSF \n           , data=val2)\n#summary(val2.lm)\n\n# Compute R-squared for each validation\n\n  # Get y-hat for each model on new data.\n  yhm &lt;- predict(val.lm, newdata=val2)\n  \n  # Compute y-bar\n  ybar &lt;- mean(val2$SalePrice) #Yi is given by Ynew from the new sample of data\n  \n  # Compute SSTO\n  SSTO &lt;- sum( (val2$SalePrice - ybar)^2 )\n  \n  # Compute SSE for each model using y - yhat\n  SSEm &lt;- sum( (val2$SalePrice - yhm)^2 )\n  \n  # Compute R-squared for each\n  rsm &lt;- 1 - SSEm/SSTO\n  \n  # Compute adjusted R-squared for each\n  n &lt;- length(val2$SalePrice) #sample size\n  \n  pm &lt;- length(coef(val.lm))\n\n  rsma &lt;- 1 - (n-1)/(n-pm)*SSEm/SSTO\n\n  \n\nmy_output_table2 &lt;- data.frame(Model = \"mylm\", `Original R2` = summary(val.lm)$r.squared , `Orig. Adj. R-squared` = summary(val.lm)$r.squared, `Validation R-squared` = rsm, `Validation Adj. R^2` = rsma)\n\ncolnames(my_output_table2) &lt;- c(\"Model\", \"Original $R^2$\", \"Original Adj. $R^2$\", \"Validation $R^2$\", \"Validation Adj. $R^2$\")\n\nknitr::kable(my_output_table2, escape=TRUE, digits=4)\n\n\n\n\n\n\n\n\n\n\n\nModel\n\n\nOriginal (R^2)\n\n\nOriginal Adj. (R^2)\n\n\nValidation (R^2)\n\n\nValidation Adj. (R^2)\n\n\n\n\n\n\nmylm\n\n\n0.7948\n\n\n0.7948\n\n\n0.7757\n\n\n0.7724"
  },
  {
    "objectID": "data_analysis/math425/House_analysis.html#graph",
    "href": "data_analysis/math425/House_analysis.html#graph",
    "title": "Housing Selling Price",
    "section": "\nGraph\n",
    "text": "Graph\n\nb &lt;- coef(mylm)\n\nplot(SalePrice ~ TotalSF, data=houses, col=OverallQual, main =\"Total number of rooms and toilets are related to Quality Impression\", yaxt=\"n\", ylab=\"SalePrice*100\")\naxis(side=2, at=seq(50000, 650000, 50000), labels=seq(50,650,50), las=2) # y axis labeling\nlegend(\"topright\", legend=c(\"Qual:high\", \"Qual:mid\", \"Qual:low\"), bty=\"n\", lty=1, col=c(\"black\", \"red\",\"green\"), cex=0.8)\nlegend(\"topleft\", legend=c(\"totaln:extra\", \"totaln:large\", \"totaln:mid\" ,\"totaln:small\"), bty=\"n\", lty=c(1, 2, 3, 4), cex=0.8)\nline &lt;- c(1,2,3,4)\n\ndrawit &lt;- function(totalNlarge=0, totalNmid=0, totalNsmall=0, OverallQuallow=0, OverallQualmid=0, i=1, j=1) {curve(b[1] + b[2]*totalNlarge + b[3]*totalNmid + b[4]*totalNsmall + b[5]*TotalSF + b[6]*OverallQuallow + b[7]*OverallQualmid + b[8]*totalNlarge*TotalSF + b[9]*totalNmid*TotalSF + b[10]*totalNsmall*TotalSF + b[11]*TotalSF*OverallQuallow + b[12]*TotalSF*OverallQualmid \n              , add=TRUE, xname =\"TotalSF\", col=palette()[i], lty=line[j])\n  \n}\n# totaln:extra\ndrawit(0,0,0,0,0,1,1) # Qual :high\ndrawit(0,0,0,0,1,2,1) # Qual :mid\ndrawit(0,0,0,1,0,3,1) # Qual :low\n\n# totaln:large\ndrawit(1,0,0,0,0,1,2) # Qual :high\ndrawit(1,0,0,0,1,2,2) # Qual :mid\ndrawit(1,0,0,1,0,3,2) # Qual :low\n\n# totaln:mid\ndrawit(0,1,0,0,0,1,3) # Qual :high\ndrawit(0,1,0,0,1,2,3) # Qual :mid\ndrawit(0,1,0,1,0,3,3) # Qual :low\n\n# totaln:small\ndrawit(0,0,1,0,0,1,4) # Qual :high\ndrawit(0,0,1,0,1,2,4) # Qual :mid\ndrawit(0,0,1,1,0,3,4) # Qual :low"
  },
  {
    "objectID": "data_analysis/math425/House_analysis.html#math-model",
    "href": "data_analysis/math425/House_analysis.html#math-model",
    "title": "Housing Selling Price",
    "section": "\nMath Model\n",
    "text": "Math Model\n\n\n[  Y_i = 0 + 1 X{totalNlarge} + 2X{totalNmid} + 3 X{totalNsmall} + 4 X{TotalSF} + 5X{OverallQuallow} + 6 X{OverallQualmid} +7 X{totalNlarge}X{TotalSF} + 8 X{totalNmid}X_{TotalSF} + 9 X{totalNsmall}X_{TotalSF} + {10} X{TotalSF}X_{OverallQuallow} + {11} X{TotalSF}X_{OverallQualmid} + _i ]"
  },
  {
    "objectID": "data_analysis/math425/House_analysis.html#interpretation-of-model",
    "href": "data_analysis/math425/House_analysis.html#interpretation-of-model",
    "title": "Housing Selling Price",
    "section": "\nInterpretation of Model\n",
    "text": "Interpretation of Model\n\n\nTotalSF is numeric value(x) to predict house prices by combining whole squared with basement, first floor and second floor. I use two switch totalN and overall quality. I calculate combination of number of rooms and number of toilets as total N and represents number of places with doors. It divided into 4 groups; extra, large, mid, and small. For example, If total N is less than 7, it is in small group. If total N is less than 15 and greater than 11, it is in larger group. Over quality also divide into 3 groups; high, mid, and low. When overall qualities are 1, 2, or 3, it is low quality. When overall qualities are 8, 9 ,or 10, it is high quality.\n\n\nThe more having total N increase the price at the same TotalSF within the same Quality impression. In the extra group, the price increases by $121.2 for each additional unit of TotalSF(beta4). Moving to the large group, there’s a decrease of $23.3(beta7) from the extra group, resulting in a price of \\(97.9(beta4+beta7) per unit of TotalSF. For the\nmid group, the price decreases of 22\\)(beta8) resulting to $99.2(beta4+beta8). For the small group, it decreases further to $26.9(beta9) resulting to $94.3(beta4+beta9).\n\n\nFor instance, consider a scenario where the total N is represented by the solid black line (totaln:extra) with a slightly higher value of 400,000 compared to the larger, mid, and small groups at 4000 Total SF. Notably, there’s not a significant disparity between large and mid total N. However, having a small total N slightly reduces the price, even with the same Total SF. For example, a house with 5000 total SF and low qualities would be priced at $170,000 for mid and large total N groups, whereas for small total N, the price drops to $150,000. This pricing trend is consistent across all situations, irrespective of quality.\n\n\nwith higher quality leading to increased prices for the same square footage. With high-quality features, the price escalates by $121.2(beta4) for each additional unit of TotalSF. However, with low-quality attributes, the price decreases by $75.1 (beta10), resulting in only a $46.1 (beta4 + beta10) increase for every unit increase in TotalSF. When the quality is mid, there’s a decrease of $45.1 (beta11). The price would increase by $76.1 (beta4 + beta11) for each unit of TotalSF.\n\n\nFor example, consider a house with a total square footage of 4000, with high-quality features, the price is around $400,000, whereas with low-quality features, it has to $150,000.\n\n\nIn summary, sellers tend to achieve higher selling prices when their property features over 7 combined rooms and toilets, or when the overall Quality impression is superior. This connection between higher selling prices and these features becomes more apparent when the property has a larger total square footage, highlighting the influence of both the number of rooms/toilets and quality on the sale price."
  },
  {
    "objectID": "data_analysis/math425/House_analysis.html#etc",
    "href": "data_analysis/math425/House_analysis.html#etc",
    "title": "Housing Selling Price",
    "section": "\netc\n",
    "text": "etc\n\n#mylm9 &lt;- lm(SalePrice~ roomn + TotalSF + roomn:TotalSF + TotalSF:TotalBT + roomn:TotalBT , data=houses)\n\n# Alley\n# houses &lt;- houses %&gt;% \n#   mutate(Alley = as.character(Alley), \n#          Alley = replace_na(Alley, \"No Alley\"),\n#          Alley = as.factor(Alley))\n\n# lm.alley &lt;- lm(SalePrice ~ Alley, data=houses)\n# summary(lm.alley)\n\n# Fence\n# houses &lt;- houses %&gt;% \n#   mutate(Fence = as.character(Fence), \n#          Fence = replace_na(Fence, \"No Fence\"),\n#          Fence = as.factor(Fence))\n# lm.fence &lt;- lm(SalePrice ~ Fence, data=houses)\n# summary(lm.fence)\n\n# total SF\n# lm.1stflr &lt;- lm(SalePrice ~ X1stFlrSF, data=houses)\n# summary(lm.1stflr)\n# \n# lm.2ndflr &lt;- lm(SalePrice ~ X2ndFlrSF, data=houses)\n# summary(lm.2ndflr)\n# \n# lm.basement &lt;- lm(SalePrice ~ TotalBsmtSF, data=houses)\n# summary(lm.basement)\n\n#lm.sqft.all &lt;- lm(SalePrice ~ X1stFlrSF + X2ndFlrSF + TotalBsmtSF, data=houses)\n# summary(lm.sqft.all)\n\n# houses &lt;- houses %&gt;%\n#   mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF)\n\n# lm.sqft &lt;- lm(SalePrice ~ TotalSF, data=houses)\n# summary(lm.sqft)\n\n \n#neighborhood\n# houses &lt;- houses %&gt;%\n#   mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF,\n#                 RichNbrhd = case_when(Neighborhood %in% c(\"StoneBr\", \"NridgHt\", \"NoRidge\") ~ 1,\n#                                                          TRUE ~ 0))\n# houses$Neighborhood\n# lm.sqft.rich &lt;- lm(SalePrice ~ TotalSF + RichNbrhd + TotalSF:RichNbrhd, data=houses)\n# summary(lm.sqft.rich)\n\n\n# HouseStyle\n# houses$HouseStyle\n\n# houses &lt;- houses %&gt;%\n#   mutate( HouseStyle = as.character(HouseStyle),\n#           HouseStyle =\n#           case_when(HouseStyle %in% c(\"1.5Fin\", \"1.5Unf\") ~ \"1Story\",\n#                     HouseStyle %in% c(\"2.5Fin\", \"2.5Unf\") ~ \"2Story\",\n#                     HouseStyle ==  \"1Story\" ~ \"1Story\",\n#                     HouseStyle == \"2Story\" ~\"2Story\",\n#                     HouseStyle %in% c('SFoyer', 'SLvl') ~ \"1Story\"),\n#           HouseStyle = as.factor(HouseStyle))\n# boxplot(SalePrice ~ HouseStyle, data=houses)\n# plot(SalePrice ~ TotalSF, data=houses, col=HouseStyle)\n# mylm &lt;- lm(SalePrice ~ HouseStyle*OverallQual*TotalSF, data=houses)\n# summary(mylm)\n# \n# \n\n#Yearreodadd\n# houses &lt;- houses %&gt;%\n#   mutate(YearRemodAdd =\n#            case_when(YearRemodAdd &lt; 1960 ~ \"1950's\",\n#                      YearRemodAdd &lt; 1970 ~ \"1960's\",\n#                      YearRemodAdd &lt; 1980 ~ \"1970's\",\n#                      YearRemodAdd &lt; 1990 ~ \"1980's\",\n#                      YearRemodAdd &lt; 2000 ~ \"1990's\",\n#                      YearRemodAdd &lt; 2010 ~ \"2000's\",\n#                      YearRemodAdd &lt; 2020 ~ \"2010's\"),\n#          YearRemodAdd = as.factor(YearRemodAdd)\n#          )\n# houses &lt;- houses %&gt;%\n#   mutate(TotalSF2 = ifelse(TotalSF&gt;8000, mean(houses$TotalSF), TotalSF))\n# boxplot(SalePrice ~ YearRemodAdd, data=houses)\n# plot(SalePrice ~ TotalSF, data=houses, col=YearRemodAdd)\n# ggplot(aes(y=SalePrice, x=TotalSF, color=as.factor(OverallQual)), data=houses)+\n#   geom_point()+\n#   facet_wrap( ~YearRemodAdd)\n# mylm2 &lt;- lm(SalePrice ~ TotalSF*OverallQual*YearRemodAdd, data=houses)\n# summary(mylm2)\n\n# mylm3 &lt;- lm(SalePrice ~ TotalSF + YearRemodAdd + OverallQual  \n#                 ,data=houses)\n# summary(mylm3)\n\n\n#GarageType, GarageYrBlt, GarageFinish, GarageCars, GarageArea, GarageQual\n# houses &lt;- houses %&gt;% \n#   mutate(GarageType = as.character(GarageType), \n#          GarageType = replace_na(GarageType, \"No GarageType\"),\n#          GarageType = as.factor(GarageType)) %&gt;% \n#   mutate(GarageQual = as.character(GarageQual), \n#          GarageQual = replace_na(GarageQual, \"No Garage\"),\n#          GarageQual = as.factor(GarageQual)) %&gt;% \n#   mutate(Garageper = GarageArea/TotalSF)\n\n\n  # mutate(GarageArea =\n  #          case_when(GarageArea &lt; 473 ~ \"small\",\n  #                    GarageArea &lt; 946 ~ \"mid\",\n  #                    GarageArea &lt; 1419 ~ \"large\"),\n  #       GarageArea = as.factor(GarageArea)\n    \n  # )\n# min(houses$GarageArea)\n# max(houses$GarageArea)\n# houses$GarageArea\n# boxplot(SalePrice~as.factor(GarageArea), data=houses)\n# mylm4 &lt;- lm(SalePrice ~ GarageArea*OverallQual*TotalSF, data=houses)\n# mylm4&lt;- lm(SalePrice ~ TotalSF +\n#              OverallQual + TotalSF:OverallQual +\n#              GarageArea + OverallQual:GarageArea +\n#              TotalSF:OverallQual:GarageArea , data=houses)\n# summary(mylm4)\n\n# mylm5&lt;- lm(SalePrice ~ GarageArea+ OverallQual +GarageArea:OverallQual   , data=houses)\n#              \n# summary(mylm5)\n\n# LotArea - GrLivArea \n# plot(SalePrice ~GrLivArea , data =houses)\n# houses &lt;- houses %&gt;% \n#   mutate(outarea =  LotArea - GrLivArea) %&gt;% \n#   mutate(outarea = case_when(\n#          outarea &lt;6272 ~ \"small\",\n#          outarea &lt;7964 ~ \"mid\",\n#          outarea &lt;9863 ~ \"large\",\n#          outarea &lt;213209 ~ \"extra large\"),\n#          outarea = as.factor(outarea))\n#   \n# houses$outarea\n# quantile(houses$outarea) #71, 6272, 7964, 9863, 213210\n# \n# mylm6&lt;- lm(SalePrice ~ GarageArea+ outarea + GarageArea:outarea   , data=houses)\n#              \n# summary(mylm6)\n\n# houses$TotalBT\n# houses &lt;- houses %&gt;%\n#   mutate(totalN = TotalBT + TotRmsAbvGrd) %&gt;%\n#   mutate(totalN = case_when(\n#          totalN &lt;7 ~ \"small\",\n#          totalN &lt;11 ~ \"mid\",\n#          totalN &lt;15 ~ \"large\",\n#          totalN &lt; 17 ~\"Extra\"),\n#          totalN = as.factor(totalN))\n# \n# boxplot(SalePrice ~ totalN, data=houses)\n# \n# mylm9 &lt;- lm(SalePrice~ totalN + TotalSF + OverallQual + totalN:TotalSF +OverallQual:TotalSF \n#             +totalN:OverallQual, data=houses)\n# summary(mylm9)\n# \n# houses &lt;- houses %&gt;% \n#   mutate(roomn = case_when(\n#          TotRmsAbvGrd &lt;6 ~ \"small\",\n#          TotRmsAbvGrd &lt;11 ~ \"mid\",\n#          TotRmsAbvGrd &lt;15 ~ \"large\"),\n#          roomn = as.factor(roomn))\n# houses &lt;- houses %&gt;%\n#   mutate(TotalBT = FullBath + HalfBath ) %&gt;% \n#   mutate(TotalBT = case_when(\n#          TotalBT &lt;2 ~ \"below_2\",\n#          TotalBT &lt;3 ~ \"2\",\n#          TotalBT &lt;5 ~ \"over_2\"),\n#          TotalBT = as.factor(TotalBT))"
  },
  {
    "objectID": "data_analysis/math425/consulting_analysis.html",
    "href": "data_analysis/math425/consulting_analysis.html",
    "title": "Combined Risk Factors for Heart Attack",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "statistics/math425/House_analysis.html",
    "href": "statistics/math425/House_analysis.html",
    "title": "Housing Selling Prices",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "database/itm111/project_course.html",
    "href": "database/itm111/project_course.html",
    "title": "University Enrollment ERD",
    "section": "",
    "text": "I designed a relational database schema using an Entity-Relationship Diagram (ERD), incorporating key entities such as students, faculty, courses, departments, and enrollments with well-defined relationships. To ensure data integrity, I structured tables efficiently to reduce redundancy. The schema supports queries that retrieve relevant information, such as identifying faculty teaching a particular student in a given term, determining the number of courses offered, analyzing faculty teaching capacities, and filtering data based on specific criteria."
  },
  {
    "objectID": "database/itm111/project_course.html#elevator-pitch",
    "href": "database/itm111/project_course.html#elevator-pitch",
    "title": "University Enrollment ERD",
    "section": "",
    "text": "….."
  },
  {
    "objectID": "database/itm111/project_course.html#erd",
    "href": "database/itm111/project_course.html#erd",
    "title": "University Enrollment ERD",
    "section": "ERD",
    "text": "ERD"
  },
  {
    "objectID": "database/itm111/project_course.html#code",
    "href": "database/itm111/project_course.html#code",
    "title": "University Enrollment ERD",
    "section": "CODE",
    "text": "CODE\n-- MySQL Workbench Forward Engineering\n\nSET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0;\nSET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0;\nSET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION';\n\n-- -----------------------------------------------------\n-- Schema university\n-- -----------------------------------------------------\nDROP SCHEMA IF EXISTS `university` ;\n\n-- -----------------------------------------------------\n-- Schema university\n-- -----------------------------------------------------\nCREATE SCHEMA IF NOT EXISTS `university` DEFAULT CHARACTER SET utf8 ;\nUSE `university` ;\n\n-- -----------------------------------------------------\n-- Table `university`.`college`\n-- -----------------------------------------------------\nDROP TABLE IF EXISTS `university`.`college` ;\n\nCREATE TABLE IF NOT EXISTS `university`.`college` (\n  `college_id` INT UNSIGNED NOT NULL AUTO_INCREMENT,\n  `college_name` VARCHAR(45) NOT NULL,\n  PRIMARY KEY (`college_id`))\nENGINE = InnoDB;\n\n\n-- -----------------------------------------------------\n-- Table `university`.`department`\n-- -----------------------------------------------------\nDROP TABLE IF EXISTS `university`.`department` ;\n\nCREATE TABLE IF NOT EXISTS `university`.`department` (\n  `department_id` INT UNSIGNED NOT NULL AUTO_INCREMENT,\n  `department_name` VARCHAR(45) NOT NULL,\n  `department_code` VARCHAR(10) NOT NULL,\n  `college_id` INT UNSIGNED NOT NULL,\n  PRIMARY KEY (`department_id`),\n  INDEX `fk_department_colleage1_idx` (`college_id` ASC) VISIBLE,\n  CONSTRAINT `fk_department_colleage1`\n    FOREIGN KEY (`college_id`)\n    REFERENCES `university`.`college` (`college_id`)\n    ON DELETE NO ACTION\n    ON UPDATE NO ACTION)\nENGINE = InnoDB;\n\n\n-- -----------------------------------------------------\n-- Table `university`.`course`\n-- -----------------------------------------------------\nDROP TABLE IF EXISTS `university`.`course` ;\n\nCREATE TABLE IF NOT EXISTS `university`.`course` (\n  `course_id` INT UNSIGNED NOT NULL AUTO_INCREMENT,\n  `course_title` VARCHAR(45) NOT NULL,\n  `course_number` INT NOT NULL,\n  `course_credits` INT NOT NULL,\n  `department_id` INT UNSIGNED NOT NULL,\n  PRIMARY KEY (`course_id`),\n  INDEX `fk_course_department1_idx` (`department_id` ASC) VISIBLE,\n  CONSTRAINT `fk_course_department1`\n    FOREIGN KEY (`department_id`)\n    REFERENCES `university`.`department` (`department_id`)\n    ON DELETE NO ACTION\n    ON UPDATE NO ACTION)\nENGINE = InnoDB;\n\n\n-- -----------------------------------------------------\n-- Table `university`.`faculty`\n-- -----------------------------------------------------\nDROP TABLE IF EXISTS `university`.`faculty` ;\n\nCREATE TABLE IF NOT EXISTS `university`.`faculty` (\n  `faculty_id` INT UNSIGNED NOT NULL AUTO_INCREMENT,\n  `faculty_fname` VARCHAR(45) NOT NULL,\n  `faculty_lname` VARCHAR(45) NOT NULL,\n  `department_id` INT UNSIGNED NOT NULL,\n  PRIMARY KEY (`faculty_id`),\n  INDEX `fk_faculty_department1_idx` (`department_id` ASC) VISIBLE,\n  CONSTRAINT `fk_faculty_department1`\n    FOREIGN KEY (`department_id`)\n    REFERENCES `university`.`department` (`department_id`)\n    ON DELETE NO ACTION\n    ON UPDATE NO ACTION)\nENGINE = InnoDB;\n\n\n-- -----------------------------------------------------\n-- Table `university`.`term`\n-- -----------------------------------------------------\nDROP TABLE IF EXISTS `university`.`term` ;\n\nCREATE TABLE IF NOT EXISTS `university`.`term` (\n  `term_id` INT UNSIGNED NOT NULL AUTO_INCREMENT,\n  `term_name` VARCHAR(45) NOT NULL,\n  `term_year` YEAR(4) NOT NULL,\n  PRIMARY KEY (`term_id`))\nENGINE = InnoDB;\n\n\n-- -----------------------------------------------------\n-- Table `university`.`section`\n-- -----------------------------------------------------\nDROP TABLE IF EXISTS `university`.`section` ;\n\nCREATE TABLE IF NOT EXISTS `university`.`section` (\n  `section_id` INT UNSIGNED NOT NULL AUTO_INCREMENT,\n  `section_number` INT NOT NULL,\n  `section_capacity` INT NOT NULL,\n  `course_id` INT UNSIGNED NOT NULL,\n  `faculty_id` INT UNSIGNED NOT NULL,\n  `term_id` INT UNSIGNED NOT NULL,\n  PRIMARY KEY (`section_id`),\n  INDEX `fk_section_course1_idx` (`course_id` ASC) VISIBLE,\n  INDEX `fk_section_faculty1_idx` (`faculty_id` ASC) VISIBLE,\n  INDEX `fk_section_term1_idx` (`term_id` ASC) VISIBLE,\n  CONSTRAINT `fk_section_course1`\n    FOREIGN KEY (`course_id`)\n    REFERENCES `university`.`course` (`course_id`)\n    ON DELETE NO ACTION\n    ON UPDATE NO ACTION,\n  CONSTRAINT `fk_section_faculty1`\n    FOREIGN KEY (`faculty_id`)\n    REFERENCES `university`.`faculty` (`faculty_id`)\n    ON DELETE NO ACTION\n    ON UPDATE NO ACTION,\n  CONSTRAINT `fk_section_term1`\n    FOREIGN KEY (`term_id`)\n    REFERENCES `university`.`term` (`term_id`)\n    ON DELETE NO ACTION\n    ON UPDATE NO ACTION)\nENGINE = InnoDB;\n\n\n-- -----------------------------------------------------\n-- Table `university`.`student`\n-- -----------------------------------------------------\nDROP TABLE IF EXISTS `university`.`student` ;\n\nCREATE TABLE IF NOT EXISTS `university`.`student` (\n  `student_id` INT UNSIGNED NOT NULL AUTO_INCREMENT,\n  `student_fname` VARCHAR(45) NOT NULL,\n  `student_lname` VARCHAR(45) NOT NULL,\n  `student_gender` CHAR(1) NOT NULL,\n  `student_city` VARCHAR(45) NULL,\n  `student_state` VARCHAR(10) NOT NULL,\n  `student_birth` DATE NOT NULL,\n  PRIMARY KEY (`student_id`))\nENGINE = InnoDB;\n\n\n-- -----------------------------------------------------\n-- Table `university`.`enrollment`\n-- -----------------------------------------------------\nDROP TABLE IF EXISTS `university`.`enrollment` ;\n\nCREATE TABLE IF NOT EXISTS `university`.`enrollment` (\n  `section_id` INT UNSIGNED NOT NULL,\n  `student_id` INT UNSIGNED NOT NULL,\n  PRIMARY KEY (`section_id`, `student_id`),\n  INDEX `fk_section_student_student1_idx` (`student_id` ASC) VISIBLE,\n  INDEX `fk_section_student_section1_idx` (`section_id` ASC) VISIBLE,\n  CONSTRAINT `fk_section_student_section1`\n    FOREIGN KEY (`section_id`)\n    REFERENCES `university`.`section` (`section_id`)\n    ON DELETE NO ACTION\n    ON UPDATE NO ACTION,\n  CONSTRAINT `fk_section_student_student1`\n    FOREIGN KEY (`student_id`)\n    REFERENCES `university`.`student` (`student_id`)\n    ON DELETE NO ACTION\n    ON UPDATE NO ACTION)\nENGINE = InnoDB;\n\n\nSET SQL_MODE=@OLD_SQL_MODE;\nSET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS;\nSET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS;\n\nuse university;\n\nINSERT INTO college (college_name)\nvalues \n('College of PhysicalScience and Engineering'),\n('College of Business and Communication'),\n('College of Language and Letters');\n\nINSERT INTO department (department_name, department_code, college_id)\nvalues\n('Computer Information Technology', 'CIT', 1),\n('Economics', 'ECON', 2),\n('Humanities and Philosophy', 'HUM', 3);\n#select*from department;\n\nINSERT INTO course (course_title, course_number, course_credits, department_id)\nvalues\n('Intro to Databases', 111, 3, 1),\n('Econometrics', 388, 4, 2),\n('Micro Economics', 150, 3, 2),\n('Classical Heritage', 376, 2, 3);\n\nINSERT INTO faculty (faculty_fname, faculty_lname, department_id)\nvalues\n('Marty', 'Morring', 1),\n('Nate', 'Norris', 2),\n('Ben', 'Barrus', 2),\n('John', 'Jensen', 3),\n('Bill', 'Barney', 1);\n\nINSERT INTO term (term_name, term_year)\nvalues\n('Fall', 2019),\n('Winter', 2018);\n\nINSERT INTO student (student_fname, student_lname, student_gender, student_city, student_state, student_birth)\nvalues\n('Paul', 'Miller', 'M', 'Dallas', 'TX', 19960222),\n('Katie', 'Smith', 'F', 'Provo', 'UT', 19950722),\n('Kelly', 'Jones', 'F', 'Provo', 'UT', 19980622),\n('Devon', 'Merrill', 'M', 'Mesa', 'AZ', 20000722),\n('Mandy', 'Murdock', 'F', 'Topeka', 'KS', 19961122),\n('Alece', 'Adams', 'F', 'Rigby', 'ID', 19970522),\n('Bryce', 'Carlson', 'M', 'Bozeman', 'MT', 19971122),\n('Preston', 'Larsen', 'M', 'Decatur', 'TN', 19960922),\n('Julia', 'Madsen', 'F', 'Rexburg', 'ID', 19980922),\n('Susan', 'Sorensen', 'F', 'Mesa', 'AZ', 19980809);\n\nINSERT INTO section (section_number, section_capacity, course_id, faculty_id, term_id)\nvalues\n(1, 30, 1, 1, 1),\n(1, 50, 3, 2, 1),\n(2, 50, 3, 2, 1),\n(1, 35, 2, 3, 1),\n(1, 30, 4, 4, 1),\n(2, 30, 1, 1, 2),\n(3, 35, 1, 5, 2),\n(1, 50, 3, 2, 2),\n(2, 50, 3, 2, 2),\n(1, 30, 4, 4, 2);\n\nINSERT INTO enrollment (section_id, student_id)\nvalues\n(7,6),\n(6,7),\n(8,7),\n(10,7),\n(5, 4),\n(9, 9),\n(4, 2),\n(4, 3),\n(4, 5),\n(5, 5),\n(1, 1),\n(3, 1),\n(9, 8),\n(6, 10);\n\n#query1\nSELECT student_fname, student_lname, date_format(student_birth, '%M %d, %Y') AS sept_birthdays\nFROM student\nWHERE date_format(student_birth, '%M') = 'September' ;\n\n#query2\nSELECT student_lname, student_fname, floor(datediff('20170105', student_birth)/365) AS 'Years', datediff('20170105', student_birth)%365 AS 'Days', concat(floor(datediff('20170105', student_birth)/365), ' - Yrs, ',datediff('20170105', student_birth)%365,' - Days') AS 'Years and Days'\nFROM student\nORDER BY student_birth ;\n\n#query3\nSELECT student_fname, student_lname\nFROM student st\nJOIN enrollment en\nON en.student_id = st.student_id\nJOIN section se\nON se.section_id = en.section_id\nJOIN faculty fa\nON fa.faculty_id = se.faculty_id\nWHERE faculty_lname = 'Jensen'\nORDER BY student_lname;\n\n#query4 \nSELECT faculty_fname, faculty_lname\nFROM student st\nJOIN enrollment en\nON en.student_id = st.student_id\nJOIN section se\nON se.section_id = en.section_id\nJOIN faculty fa\nON fa.faculty_id = se.faculty_id\nJOIN term t\nON t.term_id = se.term_id\nWHERE student_fname = 'Bryce' AND term_name ='Winter' AND term_year = 2018\nORDER BY faculty_lname;\n\n#query5\nSELECT student_fname, student_lname\nFROM student st\nJOIN enrollment en\nON en.student_id = st.student_id\nJOIN section se\nON se.section_id = en.section_id\nJOIN term t\nON t.term_id = se.term_id\nJOIN course c\nON c.course_id = se.course_id\nWHERE course_title = 'Econometrics' AND term_name ='Fall' AND term_year = 2019\nORDER BY student_lname;\n\n#query6 \nSELECT department_code, course_number, course_title\nFROM student st\nJOIN enrollment en\nON en.student_id = st.student_id\nJOIN section se\nON se.section_id = en.section_id\nJOIN term t\nON t.term_id = se.term_id\nJOIN course c\nON c.course_id = se.course_id\nJOIN department d\nON d.department_id = c.department_id\nWHERE student_lname = 'Carlson' AND term_name = 'Winter'\nORDER BY course_title;\n\n#query7\nSELECT term_name, term_year, count(st.student_id) AS Enrollment\nFROM student st\nJOIN enrollment en\nON en.student_id = st.student_id\nJOIN section se\nON se.section_id = en.section_id\nJOIN term t\nON t.term_id = se.term_id\nWHERE term_name = 'Fall' AND term_year = 2019;\n\n#query 8 \nSELECT college_name, count(c.course_id) AS courses\nFROM college cl\nJOIN department d\nON cl.college_id = d.college_id\nJOIN course c\nON d.department_id = c.department_id\nGROUP BY college_name\nORDER BY college_name;\n\n#query 9 \nSELECT faculty_fname, faculty_lname, sum(section_capacity)\nFROM faculty f\nJOIN section se\nON f.faculty_id = se.faculty_id\nJOIN term t\nON t.term_id = se.term_id\nWHERE term_name = 'Winter' AND term_year= 2018\nGROUP BY faculty_fname, faculty_lname\nORDER BY section_capacity ;\n\n#query10 \nSELECT student_lname, student_fname, sum(course_credits) AS credits\nFROM student st\nJOIN enrollment en\nON en.student_id = st.student_id\nJOIN section se\nON se.section_id = en.section_id\nJOIN course c\nON c.course_id = se.course_id\nJOIN term t\nON t.term_id = se.term_id\nWHERE term_name ='Fall' and term_year = 2019\nGROUP BY student_lname, student_fname\nHAVING credits &gt; 3\nORDER BY credits DESC;"
  },
  {
    "objectID": "database/itm111/project_course.html#goal",
    "href": "database/itm111/project_course.html#goal",
    "title": "University Enrollment ERD",
    "section": "",
    "text": "I designed a relational database schema using an Entity-Relationship Diagram (ERD), incorporating key entities such as students, faculty, courses, departments, and enrollments with well-defined relationships. To ensure data integrity, I structured tables efficiently to reduce redundancy. The schema supports queries that retrieve relevant information, such as identifying faculty teaching a particular student in a given term, determining the number of courses offered, analyzing faculty teaching capacities, and filtering data based on specific criteria."
  },
  {
    "objectID": "data_analysis/ds350/fruit_consumption.html",
    "href": "data_analysis/ds350/fruit_consumption.html",
    "title": "Fruit Consumption Analysis",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(pander)\nlibrary(ggrepel)"
  },
  {
    "objectID": "data_analysis/ds350/fruit_consumption.html#running-code",
    "href": "data_analysis/ds350/fruit_consumption.html#running-code",
    "title": "Fruit Consumption Analysis",
    "section": "Running Code",
    "text": "Running Code\n\n\nCode\ndf &lt;- read.csv(\"C:/Users/gakyeong/Documents/GitHub/DS350_WI23_Bae_Gaky/week_05/fruit-consumption-vs-gdp-per-capita.csv\")\n\ndf1 &lt;- df %&gt;%\n  rename(fruit = \"Fruit...00002919....Food.available.for.consumption...0645pc....kilograms.per.year.per.capita\", gdp = \"GDP.per.capita..PPP..constant.2017.international...\" )%&gt;% drop_na()\n  \ndf1[df1 == \"\"]&lt;-NA\n\ndf1 &lt;-df1 %&gt;% \n  filter(Year &gt;= 2015)%&gt;%\n  fill(Continent, .direction=\"down\")%&gt;%\n  filter(Year == 2019)\npander(df1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\nfruit\ngdp\nContinent\n\n\n\n\nAfghanistan\nAFG\n2019\n39.45\n2065\nAsia\n\n\nAlbania\nALB\n2019\n179.6\n13657\nEurope\n\n\nAlgeria\nDZA\n2019\n108.4\n11522\nAfrica\n\n\nAngola\nAGO\n2019\n76.34\n6670\nAfrica\n\n\nAntigua and Barbuda\nATG\n2019\n131.7\n21850\nNorth America\n\n\nArgentina\nARG\n2019\n80.54\n22066\nSouth America\n\n\nArmenia\nARM\n2019\n116\n13654\nAsia\n\n\nAustralia\nAUS\n2019\n65.07\n49309\nOceania\n\n\nAustria\nAUT\n2019\n92.75\n55834\nEurope\n\n\nAzerbaijan\nAZE\n2019\n85.5\n14442\nAsia\n\n\nBahamas\nBHS\n2019\n145.6\n36433\nNorth America\n\n\nBangladesh\nBGD\n2019\n28.76\n4754\nAsia\n\n\nBarbados\nBRB\n2019\n68.3\n15639\nNorth America\n\n\nBelarus\nBLR\n2019\n74.19\n19279\nEurope\n\n\nBelgium\nBEL\n2019\n99.09\n51944\nEurope\n\n\nBelize\nBLZ\n2019\n105.9\n7252\nNorth America\n\n\nBenin\nBEN\n2019\n43.8\n3287\nAfrica\n\n\nBolivia\nBOL\n2019\n94.48\n8724\nSouth America\n\n\nBosnia and Herzegovina\nBIH\n2019\n90.25\n14897\nEurope\n\n\nBotswana\nBWA\n2019\n26.18\n16348\nAfrica\n\n\nBrazil\nBRA\n2019\n98.92\n14764\nSouth America\n\n\nBulgaria\nBGR\n2019\n61.66\n23266\nEurope\n\n\nBurkina Faso\nBFA\n2019\n5.21\n2176\nAfrica\n\n\nBurundi\nBDI\n2019\n79.06\n751.7\nAfrica\n\n\nCambodia\nKHM\n2019\n14.99\n4389\nAsia\n\n\nCameroon\nCMR\n2019\n110.4\n3743\nAfrica\n\n\nCanada\nCAN\n2019\n96.76\n49172\nNorth America\n\n\nCape Verde\nCPV\n2019\n46.86\n7172\nAfrica\n\n\nCentral African Republic\nCAF\n2019\n52.44\n945.1\nAfrica\n\n\nChad\nTCD\n2019\n7.38\n1580\nAfrica\n\n\nChile\nCHL\n2019\n56.3\n24968\nSouth America\n\n\nChina\nCHN\n2019\n102.4\n15978\nAsia\n\n\nColombia\nCOL\n2019\n145\n14585\nSouth America\n\n\nComoros\nCOM\n2019\n32.06\n3059\nAfrica\n\n\nCongo\nCOG\n2019\n44.07\n3826\nAfrica\n\n\nCosta Rica\nCRI\n2019\n114.1\n20938\nNorth America\n\n\nCote d’Ivoire\nCIV\n2019\n67.27\n5213\nAfrica\n\n\nCroatia\nHRV\n2019\n72.99\n29336\nEurope\n\n\nCyprus\nCYP\n2019\n74.65\n41522\nEurope\n\n\nCzechia\nCZE\n2019\n55.73\n40981\nEurope\n\n\nDemocratic Republic of Congo\nCOD\n2019\n64.21\n1098\nAfrica\n\n\nDenmark\nDNK\n2019\n59.68\n57162\nEurope\n\n\nDjibouti\nDJI\n2019\n30.44\n5535\nAfrica\n\n\nDominica\nDMA\n2019\n365.6\n12369\nNorth America\n\n\nDominican Republic\nDOM\n2019\n354.8\n18413\nNorth America\n\n\nEcuador\nECU\n2019\n57.6\n11371\nSouth America\n\n\nEgypt\nEGY\n2019\n93.34\n11763\nAfrica\n\n\nEl Salvador\nSLV\n2019\n68.48\n8776\nNorth America\n\n\nEstonia\nEST\n2019\n75.44\n36401\nEurope\n\n\nEswatini\nSWZ\n2019\n94.51\n8653\nAfrica\n\n\nEthiopia\nETH\n2019\n8.81\n2221\nAfrica\n\n\nFiji\nFJI\n2019\n38.21\n13684\nOceania\n\n\nFinland\nFIN\n2019\n73.06\n48583\nEurope\n\n\nFrance\nFRA\n2019\n91.16\n46018\nEurope\n\n\nGabon\nGAB\n2019\n134.1\n14946\nAfrica\n\n\nGambia\nGMB\n2019\n6.841\n2225\nAfrica\n\n\nGeorgia\nGEO\n2019\n49.31\n14989\nAsia\n\n\nGermany\nDEU\n2019\n75.48\n53930\nEurope\n\n\nGhana\nGHA\n2019\n202.3\n5540\nAfrica\n\n\nGreece\nGRC\n2019\n144\n29698\nEurope\n\n\nGrenada\nGRD\n2019\n119.7\n16868\nNorth America\n\n\nGuatemala\nGTM\n2019\n48.54\n8653\nNorth America\n\n\nGuinea\nGIN\n2019\n89.86\n2567\nAfrica\n\n\nGuinea-Bissau\nGNB\n2019\n54.21\n1939\nAfrica\n\n\nGuyana\nGUY\n2019\n164\n13082\nSouth America\n\n\nHaiti\nHTI\n2019\n98.42\n3073\nNorth America\n\n\nHonduras\nHND\n2019\n55.77\n5736\nNorth America\n\n\nHong Kong\nHKG\n2019\n74.32\n59586\nAsia\n\n\nHungary\nHUN\n2019\n71.83\n32554\nEurope\n\n\nIceland\nISL\n2019\n87.39\n56936\nEurope\n\n\nIndia\nIND\n2019\n63.55\n6714\nAsia\n\n\nIndonesia\nIDN\n2019\n68.97\n11812\nAsia\n\n\nIran\nIRN\n2019\n144.2\n12389\nAsia\n\n\nIraq\nIRQ\n2019\n66.71\n10936\nAsia\n\n\nIreland\nIRL\n2019\n68.41\n86650\nEurope\n\n\nIsrael\nISR\n2019\n107.4\n40626\nAsia\n\n\nItaly\nITA\n2019\n123\n42708\nEurope\n\n\nJamaica\nJAM\n2019\n109\n9777\nNorth America\n\n\nJapan\nJPN\n2019\n33.35\n42022\nAsia\n\n\nJordan\nJOR\n2019\n39.81\n10071\nAsia\n\n\nKazakhstan\nKAZ\n2019\n49.3\n26352\nAsia\n\n\nKenya\nKEN\n2019\n64.68\n4453\nAfrica\n\n\nKiribati\nKIR\n2019\n73.91\n2340\nOceania\n\n\nKuwait\nKWT\n2019\n57.73\n49854\nAsia\n\n\nKyrgyzstan\nKGZ\n2019\n32.63\n5258\nAsia\n\n\nLaos\nLAO\n2019\n130.2\n7887\nAsia\n\n\nLatvia\nLVA\n2019\n49\n31012\nEurope\n\n\nLebanon\nLBN\n2019\n85.26\n14564\nAsia\n\n\nLesotho\nLSO\n2019\n16.02\n2584\nAfrica\n\n\nLiberia\nLBR\n2019\n40.14\n1470\nAfrica\n\n\nLibya\nLBY\n2019\n71.53\n15174\nAfrica\n\n\nLithuania\nLTU\n2019\n47.31\n37166\nEurope\n\n\nLuxembourg\nLUX\n2019\n86.87\n116518\nEurope\n\n\nMacao\nMAC\n2019\n57.34\n127273\nAsia\n\n\nMadagascar\nMDG\n2019\n42.9\n1619\nAfrica\n\n\nMalawi\nMWI\n2019\n190.1\n1537\nAfrica\n\n\nMalaysia\nMYS\n2019\n41.15\n28421\nAsia\n\n\nMaldives\nMDV\n2019\n71.31\n19510\nAsia\n\n\nMali\nMLI\n2019\n66.26\n2322\nAfrica\n\n\nMalta\nMLT\n2019\n80.94\n43951\nEurope\n\n\nMauritania\nMRT\n2019\n13.13\n5344\nAfrica\n\n\nMauritius\nMUS\n2019\n40.98\n22870\nAfrica\n\n\nMexico\nMEX\n2019\n120.3\n19677\nNorth America\n\n\nMoldova\nMDA\n2019\n127.2\n13027\nEurope\n\n\nMongolia\nMNG\n2019\n12.08\n12486\nAsia\n\n\nMontenegro\nMNE\n2019\n151.8\n21534\nEurope\n\n\nMorocco\nMAR\n2019\n108.5\n7547\nAfrica\n\n\nMozambique\nMOZ\n2019\n30.85\n1282\nAfrica\n\n\nMyanmar\nMMR\n2019\n48.29\n4740\nAsia\n\n\nNamibia\nNAM\n2019\n27.62\n9813\nAfrica\n\n\nNepal\nNPL\n2019\n54.71\n3953\nAsia\n\n\nNetherlands\nNLD\n2019\n101.6\n56784\nEurope\n\n\nNew Zealand\nNZL\n2019\n57.38\n42878\nOceania\n\n\nNicaragua\nNIC\n2019\n46.46\n5452\nNorth America\n\n\nNiger\nNER\n2019\n25.32\n1224\nAfrica\n\n\nNigeria\nNGA\n2019\n54.93\n5135\nAfrica\n\n\nNorth America\nNA\n2019\n110.5\n61251\nAfrica\n\n\nNorth Macedonia\nMKD\n2019\n113.6\n16773\nEurope\n\n\nNorway\nNOR\n2019\n77.24\n64385\nEurope\n\n\nOman\nOMN\n2019\n134.1\n31284\nAsia\n\n\nPakistan\nPAK\n2019\n35.6\n4698\nAsia\n\n\nPanama\nPAN\n2019\n76.47\n31440\nNorth America\n\n\nPapua New Guinea\nPNG\n2019\n259.8\n4293\nOceania\n\n\nParaguay\nPRY\n2019\n76.08\n12616\nSouth America\n\n\nPeru\nPER\n2019\n130.9\n12854\nSouth America\n\n\nPhilippines\nPHL\n2019\n101\n8915\nAsia\n\n\nPoland\nPOL\n2019\n61.83\n33185\nEurope\n\n\nPortugal\nPRT\n2019\n131.6\n34946\nEurope\n\n\nRomania\nROU\n2019\n107.1\n29875\nEurope\n\n\nRussia\nRUS\n2019\n61.24\n27211\nEurope\n\n\nRwanda\nRWA\n2019\n179.2\n2228\nAfrica\n\n\nSaint Kitts and Nevis\nKNA\n2019\n68.42\n28603\nNorth America\n\n\nSaint Lucia\nLCA\n2019\n87.97\n15201\nNorth America\n\n\nSaint Vincent and the Grenadines\nVCT\n2019\n153.9\n12489\nNorth America\n\n\nSamoa\nWSM\n2019\n177.3\n6632\nOceania\n\n\nSao Tome and Principe\nSTP\n2019\n210.3\n4005\nAfrica\n\n\nSaudi Arabia\nSAU\n2019\n69.65\n46962\nAsia\n\n\nSenegal\nSEN\n2019\n19.36\n3361\nAfrica\n\n\nSerbia\nSRB\n2019\n105\n18307\nEurope\n\n\nSeychelles\nSYC\n2019\n86.19\n27611\nAfrica\n\n\nSierra Leone\nSLE\n2019\n33.74\n1705\nAfrica\n\n\nSlovakia\nSVK\n2019\n51.61\n31928\nEurope\n\n\nSlovenia\nSVN\n2019\n95.6\n38947\nEurope\n\n\nSolomon Islands\nSLB\n2019\n42.1\n2661\nOceania\n\n\nSouth Africa\nZAF\n2019\n27.14\n13710\nAfrica\n\n\nSouth Korea\nKOR\n2019\n52.2\n42759\nAsia\n\n\nSpain\nESP\n2019\n86.24\n40802\nEurope\n\n\nSri Lanka\nLKA\n2019\n34.97\n13070\nAsia\n\n\nSudan\nSDN\n2019\n65.78\n4174\nAfrica\n\n\nSuriname\nSUR\n2019\n84.8\n19037\nSouth America\n\n\nSweden\nSWE\n2019\n59.33\n52851\nEurope\n\n\nSwitzerland\nCHE\n2019\n80.39\n70944\nEurope\n\n\nTajikistan\nTJK\n2019\n38.6\n3581\nAsia\n\n\nTanzania\nTZA\n2019\n75.93\n2661\nAfrica\n\n\nThailand\nTHA\n2019\n77.51\n18453\nAsia\n\n\nTimor\nTLS\n2019\n14.87\n3627\nAsia\n\n\nTogo\nTGO\n2019\n8.13\n2122\nAfrica\n\n\nTrinidad and Tobago\nTTO\n2019\n55.02\n25828\nNorth America\n\n\nTunisia\nTUN\n2019\n100.6\n11417\nAfrica\n\n\nTurkey\nTUR\n2019\n128.2\n28197\nAsia\n\n\nTurkmenistan\nTKM\n2019\n55.94\n15538\nAsia\n\n\nUganda\nUGA\n2019\n171.9\n2183\nAfrica\n\n\nUkraine\nUKR\n2019\n61.14\n12805\nEurope\n\n\nUnited Arab Emirates\nARE\n2019\n56.07\n68264\nAsia\n\n\nUnited Kingdom\nGBR\n2019\n78.78\n47369\nEurope\n\n\nUnited States\nUSA\n2019\n106.7\n62631\nNorth America\n\n\nUruguay\nURY\n2019\n76.34\n23033\nSouth America\n\n\nUzbekistan\nUZB\n2019\n109.4\n7348\nAsia\n\n\nVanuatu\nVUT\n2019\n73.35\n3137\nOceania\n\n\nVietnam\nVNM\n2019\n74.91\n8041\nAsia\n\n\nWorld\nOWID_WRL\n2019\n78.72\n16897\nAsia\n\n\nZambia\nZMB\n2019\n7.59\n3470\nAfrica\n\n\nZimbabwe\nZWE\n2019\n14.33\n3630\nAfrica\n\n\n\n\n\n\n\nCode\ngraph1 &lt;- ggplot(data=df1, mapping=aes(x=gdp, y=fruit, color =Continent))+\n  geom_point()+\n  scale_x_continuous(trans = \"log\",\n                     breaks = c(1000,2000,5000,10000,20000,50000,100000),\n                     limits = c(1000, 100000),\n                     labels = scales::dollar_format(accuracy =1))+\n  scale_y_continuous(breaks = seq(0, 350, 50),\n                     labels=c(\"0 kg\",\"50 kg\", \"100 kg\", \"150 kg\", \"200 kg\", \"250 kg\",\"300 kg\", \"350 kg\"),\n                    #unit_format(suffix =\"kg\")\n                    #labels= function(y)(paste(y,\"kg\", sep=\" \")\n                    #labels = `paste(.x, \"kg\", sep =\" \")\n                     expand = expansion(mult = c(0.015, .05)) # default is .05 is either end\n                    \n                    )+\n  theme_minimal()+\n  theme(axis.line.x.bottom = element_line(color = \"black\"),\n        panel.grid = element_line(linetype = \"dashed\"),\n        panel.grid.minor = element_blank(),\n        legend.justification = \"top\",\n        plot.caption = element_text(hjust = 0),\n        plot.title.position = \"plot\",\n        plot.caption.position = \"plot\"\n        )+\n  guides(color= guide_legend(override.aes = list(shape = 15, size = 3)))+\n  labs(color = NULL, \n       title = \"Fruit consumption vs GDP per capita, 2019\",\n       subtitle = \"Average per capita fruit consumption, mesured in kilograms per year versus gross domestic product(GDP) per capita, measured in constant international-$.\",\n       x=\"GDP per capita\",\n       y=\"Fruit supply per person\",\n       caption = \"Source: Food and Agriculture Organization of the United Nations, Data compiled from multiple sources by World Bank OurWorldInData.org/edit-compositions. CC BY\")\n\ngraph1\n\n\n\n\n\n\n\n\n\n\ngraph2\n\n\nCode\np1 &lt;- df1%&gt;%\n  filter(Entity %in% c(\"China\", \"South Korea\", \"Japan\"))\n\n#graph1 + geom_label(p1, mapping =aes(gdp, fruit, label = Entity), nudge_x = 0.25,nudge_y = 0.25, check_overlap = T)\n\ngraph2 &lt;- graph1+geom_label_repel(p1, mapping =aes(gdp, fruit, label = Entity), nudge_x = 0.25,nudge_y = 0.25)+ labs(title = \"Fruit Consumtpion of Three East Asia countries\", subtitle = \"\")\ngraph2\n\n\n\n\n\n\n\n\n\ngraph2) I tried to highlights to three countries which are China, South Korea, Japan.\n\n\ngraph3\n\n\nCode\np2 &lt;- df1%&gt;%\n  filter(Continent ==\"Asia\")\n\ngraph3 &lt;- graph1 +scale_colour_grey()+\n  geom_point(data=p2, mapping=aes(x=gdp, y=fruit), color = \"red\")+\n  labs(title = \"Fruit Consumption in Asia\", subtitle = \"\")+\n  annotate(geom=\"text\", x=50000, y=250, label=\"Asia\",\n              color=\"red\", fontface=\"italic\", size= 13)\ngraph3\n\n\n\n\n\n\n\n\n\ngraph3) I emphasizes on continent “Asia”."
  },
  {
    "objectID": "data_analysis/ds350/final project.html",
    "href": "data_analysis/ds350/final project.html",
    "title": "Education Equality in teacher ratio",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggrepel)\nlibrary(readxl)\nlibrary(stringr)\n# install.packages(\"geofacet\")\nlibrary(geofacet)\nlibrary(USAboundaries)\nlibrary(USAboundariesData)\nlibrary(sf)"
  },
  {
    "objectID": "data_analysis/ds350/final project.html#introduction",
    "href": "data_analysis/ds350/final project.html#introduction",
    "title": "Education Equality in teacher ratio",
    "section": "Introduction",
    "text": "Introduction\nEducation attainment refers to the highest level of education that an individual has completed. Inequalities in education is problematic in society level because education achievement usually leads to economic prospects. According to UNICEF, the lack of qualified teachers, inadequate teaching materials, and poor sanitation are some of the reasons why children do not receive a quality education. Since house income have been enough to support clean sanitation in US, we want to know more about teacher ratio to analysis inadequate teaching materials and lack of qualified teachers regarding the equality of education. All date comes from National Center for Education Statistics (NCES)."
  },
  {
    "objectID": "data_analysis/ds350/final project.html#data-visualization-1-school-drop-rate",
    "href": "data_analysis/ds350/final project.html#data-visualization-1-school-drop-rate",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 1 : school drop rate",
    "text": "Data & Visualization 1 : school drop rate\n\n\nCode\ndrop_data &lt;- read_xlsx(\"C:/Users/gakyeong/Desktop/2022.4th/project/drop out.xlsx\",sheet = 3) \n# rename columns, substr\ncolnames(drop_data)[1] =\"year\" ; colnames(drop_data)[2] = \"All_races\"\ncolnames(drop_data)[4] =\"White\" ; colnames(drop_data)[6] =\"Black\"; colnames(drop_data)[8] =\"Hispanic\"\n\ndrop_data$year &lt;- substr(drop_data$year, 0, 4)\n\ndrop_data1 &lt;- drop_data %&gt;% \n  select(year, All_races, White, Black, Hispanic) %&gt;% \n  filter(year &gt;=1975) %&gt;% \n  mutate(Hispanic = as.numeric(Hispanic), year = as.numeric(year))\n\n\n\n\nCode\nggplot()+\n  geom_line(data = drop_data1, aes(x =year, y= All_races), color = \"cyan3\", size =1.5)+\n  geom_line(data = drop_data1, aes(x =year, y= White), color = \"cornsilk\", size =1.5)+\n  geom_line(data = drop_data1, aes(x =year, y= Black),  color = \"darkgrey\", size =1.5)+\n  geom_line(data = drop_data1, aes(x =year, y= Hispanic),  color = \"darkgoldenrod1\", size =1.5)+\n  theme_bw()+\n  annotate(geom = \"text\", x = 2018, y = 2300, label = \"All races\", hjust = \"left\", color= \"cyan3\")+\n  labs(title = \"The Number of school drop by ethnicity\" , y=\"\")+\n  theme(plot.title = element_text(face = \"bold\", family = \"serif\"))\n\n\n\n\n\n\n\n\n\nIt can be concluded that as high school dropout rates decrease, there is an increase in educational attainment over time. Furthermore, since 2020, the number of school dropouts has decreased irrespective of the student’s ethnicity.\nIt makes me to wonders that not only decreasing number of drop out in secondary school, but also how students get the qualified education. For these reason, one of the assessment on education quality is that one teacher care how many students in a class."
  },
  {
    "objectID": "data_analysis/ds350/final project.html#data-visualization-2-general-teacher-ratio-information",
    "href": "data_analysis/ds350/final project.html#data-visualization-2-general-teacher-ratio-information",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 2: General teacher ratio information",
    "text": "Data & Visualization 2: General teacher ratio information\n\n\nCode\nteacher_ratio &lt;- read_xls(\"C:/Users/gakyeong/Desktop/2022.4th/project/teacher ratio.xls\", skip=2)\ncolnames(teacher_ratio)[1] =\"State\"\n\nteacher &lt;-teacher_ratio %&gt;% \n  filter(!State %in% c('Puerto Rico', 'District of Columbia', \"United States\")) %&gt;% \n  select (State,\"2\",\"3\",\"4\",\"5\",\"6\", \"9\", \"12\",\"13\",\"14\", \"15\") %&gt;% \n  group_by(State) %&gt;% \n  arrange(State) %&gt;% \n  mutate(state_abb = state.abb[match(State, state.name)]) %&gt;% \n  rename(\"2000\"=`2`,\"2005\"=`3`, \"2010\"=`4`, \"2015\"=`5`, \"2016\"=`6`, \"2017\" =`9`, \"2018\" = `12`, \"2019\" = `15`, \"2019_teacher\" = `13`, \"2019_enrollment\" = `14`) \n\nteacher1 &lt;- teacher %&gt;% \n  select(State, \"2000\",\"2005\", \"2010\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\") %&gt;% \n  mutate(state_abb = state.abb[match(State, state.name)]) %&gt;% \n  pivot_longer(cols= c('2000':'2019'), names_to = \"year\", values_to = \"value\" ,values_drop_na = TRUE)\n\nteacher1&lt;-teacher1 %&gt;% mutate(diff = (value - mean(teacher1$value)))\n\n\n\n\nCode\nteacher2 &lt;- teacher %&gt;% \n  select(State, \"2019_enrollment\", \"2019_teacher\", \"2019\" ) %&gt;% \n  arrange(`2019_enrollment`) %&gt;% \n  na.omit()\n\ntext_point &lt;- teacher2 %&gt;% filter(State %in% c(\"North Dakota\",\"West Virginia\", \"Vermont\", \"Connecticut\"))\n\nggplot(teacher2, aes(x= `2019_enrollment`, y=`2019_teacher`)) + \n  geom_point(aes(color = State))+\n  geom_smooth(method = \"lm\", se = FALSE, color= \"orange\")+\n  scale_x_continuous(trans = \"log\")+\n  scale_y_continuous(trans = \"log\")+\n  theme_bw()+\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\", family = \"serif\"),\n        plot.subtitle = element_text(color =\"grey40\", face=\"italic\", angle =0.5))+\n  labs(title = \"Strong regression on number of enrollment and teachers in 2019\", x=\"Enrollment\", y =\"Teachers\", subtitle = \"The higher number of students requires the higher number of teacher \")+\n  geom_label_repel(data=text_point, aes(x= `2019_enrollment`, y=`2019_teacher`, label= State), color = \"blue\", size = 2)\n\n\n\n\n\n\n\n\n\nThe mean of teacher ratio is 15.49 over times. One teacher take care 16 people in a class on average. The above graph shows that enrollment have a strong relationship with the number of teachers. school tend to hire more teachers when they have many children in a class. This means that school try to keep the teacher’s ratio."
  },
  {
    "objectID": "data_analysis/ds350/final project.html#data-visualization-2-1-teacher-ratio-overtimes.",
    "href": "data_analysis/ds350/final project.html#data-visualization-2-1-teacher-ratio-overtimes.",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 2-1 : teacher ratio overtimes.",
    "text": "Data & Visualization 2-1 : teacher ratio overtimes.\n\n\nCode\nteacher5&lt;- teacher1 %&gt;% \n  mutate(group_teacher = case_when(\n  `value` &lt; 12 ~ \"group1\",\n  `value` &lt; 13  & `value` &gt;= 12 ~ \"group2\",\n  `value` &lt; 14  & `value` &gt;= 13 ~ \"group3\",\n  `value` &lt; 15  & `value` &gt;= 14 ~ \"group4\",\n  `value` &gt;= 15   ~ \"group5\"\n)) %&gt;% \n  filter(year==\"2000\"| year ==\"2010\")\n\nggplot(teacher5, mapping =aes(x = value, y = state_abb)) +\n  geom_path(mapping = aes(group = state_abb), \n             arrow = arrow(length = unit(0.15, \"cm\")))+\n  geom_point(mapping =aes(color = year))+\n  theme(\n    plot.margin=unit(c( -7, -1, -4 , 1), 'cm'),\n    legend.position = \"left\"\n  )\n\n\n\n\n\n\n\n\n\nBetween 2000 and 2010, there was an improvement in teacher ratios across most states. Notably, FL, NH, and ND experienced the most significant improvements, with each teacher responsible for fewer students. However, in NV and IN, the number of students per teacher increased, indicating a decline in teacher ratios."
  },
  {
    "objectID": "data_analysis/ds350/final project.html#data-visualization-3-teacher-ratio-by-state",
    "href": "data_analysis/ds350/final project.html#data-visualization-3-teacher-ratio-by-state",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 3: Teacher Ratio by state",
    "text": "Data & Visualization 3: Teacher Ratio by state\n\n\nCode\n# ggplot(teacher1, aes(year, value))+\n#   geom_boxplot()+\n#   geom_jitter(aes(colour = State), width = 0.25)+\n#   theme(legend.position = \"none\")+\n#   geom_text_repel(teacher1, mapping = aes(label=state_abb))\n\n# ggplot(teacher1, aes(as.numeric(year), value)) +\n#   geom_line() +\n#   facet_geo(~ state_abb, grid = \"us_state_grid2\") +\n#   scale_x_continuous(labels = function(x) paste0(\"'\", substr(x, 3, 4))) +\n#   ylab(\"Teacher Rate (%)\")+\n#   theme_bw()+\n#   labs(title = \"Teacher ratio by State\", x=\"year\", y =\"Teacher ratio\", caption = \"This data starts from 2000 to 2019\")+\n#   theme(legend.position = \"none\",\n#         plot.title = element_text(face = \"bold\", family = \"serif\"),\n#         plot.caption = element_text(color =\"grey40\", face=\"italic\", angle =0.5))\n\nggplot(teacher1, aes(as.numeric(year), diff)) +\n  geom_bar(stat= \"identity\") +\n  geom_hline(yintercept = 0, color= \"orange\")+\n  facet_geo(~ state_abb, grid = \"us_state_grid2\") +\n  scale_x_continuous(labels = function(x) paste0(\"'\", substr(x, 3, 4)),\n                     breaks = c(2000, 2010, 2020)) +\n  ylab(\"Teacher Rate (%)\")+\n  theme_bw()+\n  labs(title = \"Teacher ratio by State\", x=\"year\", y =\"Teacher ratio\", caption = \"This data starts from 2000 to 2019\")+\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\", family = \"serif\"),\n        plot.caption = element_text(color =\"grey40\", face=\"italic\", angle =0.5),\n        strip.text.x = element_text(size = 6))\n\n\n\n\n\n\n\n\n\nThe graph illustrates the states that have a higher teacher ratio than the average. It is interesting to note that western states are more densely populated and have a higher teacher ratio, meaning that each teacher is responsible for a larger number of students. Conversely, eastern and northern states have a lower teacher ratio."
  },
  {
    "objectID": "data_analysis/ds350/final project.html#data-visualization-3-2-teacher-ratio-by-state",
    "href": "data_analysis/ds350/final project.html#data-visualization-3-2-teacher-ratio-by-state",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 3-2: teacher ratio by state",
    "text": "Data & Visualization 3-2: teacher ratio by state\n\n\nCode\nstate_shape &lt;- us_states() %&gt;% \n  filter(jurisdiction_type == \"state\", \n         state_abbr != \"AK\", \n         state_abbr != \"HI\")\nstate_shape$state_abbr\n\n\n [1] \"CA\" \"WI\" \"ID\" \"MN\" \"IA\" \"MO\" \"MD\" \"OR\" \"MI\" \"MT\" \"UT\" \"VA\" \"IL\" \"TX\" \"LA\"\n[16] \"FL\" \"KY\" \"TN\" \"NY\" \"GA\" \"DE\" \"WA\" \"OK\" \"WV\" \"PA\" \"CO\" \"NC\" \"IN\" \"KS\" \"RI\"\n[31] \"WY\" \"MS\" \"MA\" \"CT\" \"VT\" \"ME\" \"NH\" \"NM\" \"OH\" \"AL\" \"NE\" \"SD\" \"NV\" \"NJ\" \"ND\"\n[46] \"SC\" \"AZ\" \"AR\"\n\n\nCode\nteacher3&lt;- teacher2 %&gt;% mutate(group_edu = case_when(\n  `2019` &lt; 12 ~ \"group1\",\n  `2019` &lt; 13  & `2019` &gt;= 12 ~ \"group2\",\n  `2019` &lt; 14  & `2019` &gt;= 13 ~ \"group3\",\n  `2019` &lt; 15  & `2019` &gt;= 14 ~ \"group4\",\n  `2019` &gt;= 15   ~ \"group5\"\n)) %&gt;% \n  mutate(state_abbr = state.abb[match(State, state.name)]) %&gt;% \n  filter(state_abbr != \"AK\", \n         state_abbr != \"HI\") %&gt;% \n  arrange(`2019`)\n\nteacher4 &lt;- full_join(state_shape, teacher3, by =\"state_abbr\")\n\nggplot() +\n  geom_sf(data = state_shape)+\n  geom_sf(data= teacher4, aes(fill=group_edu, geometry = geometry),  stat = \"sf_coordinates\", alpha = 0.6)+\n  theme_minimal()+\n  theme(panel.border = element_rect(color=\"black\", fill =NA))+\n  theme(legend.position = \"bottom\")+\n  geom_sf_text(data= teacher4, aes(label = state_abbr), size = 2)\n\n\n\n\n\n\n\n\n\nThe graphs above illustrate the difference in teacher ratios from the average, categorized into five groups for values higher than the mean. It is evident from the graphs that Vermont has the lowest teacher ratio.\nQ. What is the reason east north have lower teacher ratio?\n\nData & Visualization 4: teacher salary by state\n\n\nCode\nteacher_salary &lt;- read_xls(\"C:/Users/gakyeong/Desktop/2022.4th/project/teacher salary.xls\", skip=2)\ncolnames(teacher_salary)[1] =\"State\"\n\nteacher_salary &lt;-teacher_salary %&gt;% \n  filter(!State %in% c('Puerto Rico', 'District of Columbia', \"United States\")) %&gt;% \n  select (State,\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\", \"9\") %&gt;% \n  group_by(State) %&gt;% \n  arrange(State) %&gt;% \n  mutate(state_abb = state.abb[match(State, state.name)]) %&gt;% \n  rename(\"1970\"=`2`,\"1980\"=`3`, \"1990\"=`4`, \"2000\"=`5`, \"2010\"=`6`, \"2020\" = `7`, \"2021\" =`8`, \"2022\" = `9`)\n\nTea_sal1 &lt;- teacher_salary %&gt;% \n  pivot_longer(cols= c('1970':'2022'), names_to = \"year\", values_to = \"value\" ,values_drop_na = TRUE)\n\nUS_avg &lt;-Tea_sal1 %&gt;% \n  group_by(year) %&gt;% \n  summarise(average = median(value))\n\nTop5_ratio&lt;- Tea_sal1  %&gt;% filter(state_abb %in% c(\"VT\", \"NJ\", \"NH\", \"ME\", \"CT\"))\nTop5_ratio1&lt;- Tea_sal1  %&gt;% filter(state_abb %in% c(\"VT\", \"NJ\", \"NH\", \"ME\", \"CT\"),\n                                   year == \"2022\")\n\nVT &lt;- Tea_sal1 %&gt;% filter(state_abb == \"VT\")\nNJ &lt;- Tea_sal1 %&gt;% filter(state_abb == \"NJ\")\nNH &lt;- Tea_sal1 %&gt;% filter(state_abb == \"NH\")\nME &lt;- Tea_sal1 %&gt;% filter(state_abb == \"ME\")\nCT &lt;- Tea_sal1 %&gt;% filter(state_abb == \"CT\")\n\noptions(scipen = 999)\nlibrary(scales)\n\nggplot()+\n  geom_point(data = Tea_sal1, aes(x= year, y=value),color=\"grey\")+\n  geom_point(data=US_avg, aes(x= year, y=average), size = 2.3)+\n  geom_segment(data = US_avg, aes(x = year,\n                                  y=average,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(average, n=-1), NA)),\n               size = 0.8)+\n  geom_point(data= Top5_ratio, aes(x= year, y=value), color=\"red\", fill= \"white\", size =1.5)+\n  geom_segment(data = VT, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  geom_segment(data = NJ, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  geom_segment(data = NH, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  geom_segment(data = ME, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  geom_segment(data = CT, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  theme_bw()+\n  theme(legend.position = \"none\")+\n  labs(title = \"History of teacher's salary by state\", y=\"Annual Salary ($)\")+\n  scale_y_continuous(labels = scales::comma)+\n  geom_text_repel(data= Top5_ratio1, \n                  aes(x= year, y=value, label= state_abb),\n                  direction = \"y\", \n                  hjust = -1)\n\n\n\n\n\n\n\n\n\nTo investigate the reason behind the lower teacher ratio in the east-northern states, I am interested in exploring the relationship between teacher ratio and teacher salary. The black line above represents the average salary of teachers in the US, indicating a sharp increase in annual salary between 1980 and 2010.\n\n\nData & Visualization 4-2: Teacher salary over time\n\n\nCode\nlibrary(leaflet)\n\nmax(Tea_sal1$value) - mean(Tea_sal1$value)\n\n\n[1] 51120.88\n\n\nCode\nTea_sal2 &lt;-Tea_sal1 %&gt;% \n  rename(\"state_abbr\"= state_abb) %&gt;% \n  filter(state_abbr != \"AK\", \n         state_abbr != \"HI\") %&gt;% \n  mutate(group_sal = case_when(\n  value &gt;= mean(Tea_sal1$value)+40000 ~ \"group1\",\n  value &lt; mean(Tea_sal1$value)+40000 & value &gt;= mean(Tea_sal1$value)+30000 ~ \"group2\",\n  value &lt; mean(Tea_sal1$value)+30000 & value &gt;= mean(Tea_sal1$value)+20000 ~ \"group3\",\n  value &lt; mean(Tea_sal1$value)+20000 & value &gt;= mean(Tea_sal1$value)+10000 ~ \"group4\",\n  value &lt; mean(Tea_sal1$value)+10000 & value &gt;= mean(Tea_sal1$value) ~ \"group5\", \n  value &lt; mean(Tea_sal1$value) ~\"group6\"\n))\n\nTea_sal3 &lt;- inner_join(state_shape, Tea_sal2, by=\"state_abbr\")\n\n\n\n\nCode\npal &lt;- colorNumeric(palette = c(\"white\",\"orange\", \"red\"),\n                    domain = min(Tea_sal3$value):max(Tea_sal3$value))\n\n\n\n\nCode\nleaflet() %&gt;% \n  setView(lng = -99, lat = 40, zoom = 4) %&gt;% \n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;%\n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"1970\")),\n              group = \"1970\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"1980\")),\n              group = \"1980\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"1990\")),\n              group = \"1990\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"2000\")),\n              group = \"2000\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"2010\")),\n              group = \"2010\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"2020\")),\n              group = \"2020\",\n              fillOpacity = .5,\n              fillColor = ~pal(value))%&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"2021\")),\n              group = \"2021\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addLayersControl(\n    baseGroups = c(\"1970\", \"1980\", \"1990\", \"2000\", \"2010\",\"2020\", \"2021\"),\n    options = layersControlOptions(collapsed = FALSE)) \n\n\n\n\n\n\nOver time, it has been demonstrated that teacher salaries have increased. However, teacher salary is not the only factor that affects teacher ratio. It has been observed that the eastern and northern states of CT, NJ, VT, and NH, which also have lower teacher ratios, offer higher annual salaries to teachers. Conversely, ME has a lower teacher ratio but offers the same salary as the average."
  },
  {
    "objectID": "data_analysis/ds350/final project.html#conclusion",
    "href": "data_analysis/ds350/final project.html#conclusion",
    "title": "Education Equality in teacher ratio",
    "section": "Conclusion",
    "text": "Conclusion\nIn general, there has been a decrease in teacher ratios, allowing students to receive more personalized education in a classroom setting. Notably, in the eastern and northern states, teacher salary appears to be related to teacher ratios to some extent, with states offering higher salaries experiencing lower ratios. However, further detailed data is needed to determine the other factors that influence the lower-than-average teacher ratios in these states."
  },
  {
    "objectID": "data_analysis/ds350/teacher_ratio.html",
    "href": "data_analysis/ds350/teacher_ratio.html",
    "title": "Education Equality in teacher ratio",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggrepel)\nlibrary(readxl)\nlibrary(stringr)\n# install.packages(\"geofacet\")\nlibrary(geofacet)\nlibrary(USAboundaries)\nlibrary(USAboundariesData)\nlibrary(sf)"
  },
  {
    "objectID": "data_analysis/ds350/teacher_ratio.html#introduction",
    "href": "data_analysis/ds350/teacher_ratio.html#introduction",
    "title": "Education Equality in teacher ratio",
    "section": "Introduction",
    "text": "Introduction\nEducation attainment refers to the highest level of education that an individual has completed. Inequalities in education is problematic in society level because education achievement usually leads to economic prospects. According to UNICEF, the lack of qualified teachers, inadequate teaching materials, and poor sanitation are some of the reasons why children do not receive a quality education. Since house income have been enough to support clean sanitation in US, we want to know more about teacher ratio to analysis inadequate teaching materials and lack of qualified teachers regarding the equality of education. All date comes from National Center for Education Statistics (NCES)."
  },
  {
    "objectID": "data_analysis/ds350/teacher_ratio.html#data-visualization-1-school-drop-rate",
    "href": "data_analysis/ds350/teacher_ratio.html#data-visualization-1-school-drop-rate",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 1 : school drop rate",
    "text": "Data & Visualization 1 : school drop rate\n\n\nCode\ndrop_data &lt;- read_xlsx(\"C:/Users/gakyeong/Desktop/2022.4th/project/drop out.xlsx\",sheet = 3) \n# rename columns, substr\ncolnames(drop_data)[1] =\"year\" ; colnames(drop_data)[2] = \"All_races\"\ncolnames(drop_data)[4] =\"White\" ; colnames(drop_data)[6] =\"Black\"; colnames(drop_data)[8] =\"Hispanic\"\n\ndrop_data$year &lt;- substr(drop_data$year, 0, 4)\n\ndrop_data1 &lt;- drop_data %&gt;% \n  select(year, All_races, White, Black, Hispanic) %&gt;% \n  filter(year &gt;=1975) %&gt;% \n  mutate(Hispanic = as.numeric(Hispanic), year = as.numeric(year))\n\n\n\n\nCode\nggplot()+\n  geom_line(data = drop_data1, aes(x =year, y= All_races), color = \"cyan3\", size =1.5)+\n  geom_line(data = drop_data1, aes(x =year, y= White), color = \"cornsilk\", size =1.5)+\n  geom_line(data = drop_data1, aes(x =year, y= Black),  color = \"darkgrey\", size =1.5)+\n  geom_line(data = drop_data1, aes(x =year, y= Hispanic),  color = \"darkgoldenrod1\", size =1.5)+\n  theme_bw()+\n  annotate(geom = \"text\", x = 2018, y = 2300, label = \"All races\", hjust = \"left\", color= \"cyan3\")+\n  labs(title = \"The Number of school drop by ethnicity\" , y=\"\")+\n  theme(plot.title = element_text(face = \"bold\", family = \"serif\"))\n\n\n\n\n\n\n\n\n\nIt can be concluded that as high school dropout rates decrease, there is an increase in educational attainment over time. Furthermore, since 2020, the number of school dropouts has decreased irrespective of the student’s ethnicity.\nIt makes me to wonders that not only decreasing number of drop out in secondary school, but also how students get the qualified education. For these reason, one of the assessment on education quality is that one teacher care how many students in a class."
  },
  {
    "objectID": "data_analysis/ds350/teacher_ratio.html#data-visualization-2-general-teacher-ratio-information",
    "href": "data_analysis/ds350/teacher_ratio.html#data-visualization-2-general-teacher-ratio-information",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 2: General teacher ratio information",
    "text": "Data & Visualization 2: General teacher ratio information\n\n\nCode\nteacher_ratio &lt;- read_xls(\"C:/Users/gakyeong/Desktop/2022.4th/project/teacher ratio.xls\", skip=2)\ncolnames(teacher_ratio)[1] =\"State\"\n\nteacher &lt;-teacher_ratio %&gt;% \n  filter(!State %in% c('Puerto Rico', 'District of Columbia', \"United States\")) %&gt;% \n  select (State,\"2\",\"3\",\"4\",\"5\",\"6\", \"9\", \"12\",\"13\",\"14\", \"15\") %&gt;% \n  group_by(State) %&gt;% \n  arrange(State) %&gt;% \n  mutate(state_abb = state.abb[match(State, state.name)]) %&gt;% \n  rename(\"2000\"=`2`,\"2005\"=`3`, \"2010\"=`4`, \"2015\"=`5`, \"2016\"=`6`, \"2017\" =`9`, \"2018\" = `12`, \"2019\" = `15`, \"2019_teacher\" = `13`, \"2019_enrollment\" = `14`) \n\nteacher1 &lt;- teacher %&gt;% \n  select(State, \"2000\",\"2005\", \"2010\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\") %&gt;% \n  mutate(state_abb = state.abb[match(State, state.name)]) %&gt;% \n  pivot_longer(cols= c('2000':'2019'), names_to = \"year\", values_to = \"value\" ,values_drop_na = TRUE)\n\nteacher1&lt;-teacher1 %&gt;% mutate(diff = (value - mean(teacher1$value)))\n\n\n\n\nCode\nteacher2 &lt;- teacher %&gt;% \n  select(State, \"2019_enrollment\", \"2019_teacher\", \"2019\" ) %&gt;% \n  arrange(`2019_enrollment`) %&gt;% \n  na.omit()\n\ntext_point &lt;- teacher2 %&gt;% filter(State %in% c(\"North Dakota\",\"West Virginia\", \"Vermont\", \"Connecticut\"))\n\nggplot(teacher2, aes(x= `2019_enrollment`, y=`2019_teacher`)) + \n  geom_point(aes(color = State))+\n  geom_smooth(method = \"lm\", se = FALSE, color= \"orange\")+\n  scale_x_continuous(trans = \"log\")+\n  scale_y_continuous(trans = \"log\")+\n  theme_bw()+\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\", family = \"serif\"),\n        plot.subtitle = element_text(color =\"grey40\", face=\"italic\", angle =0.5))+\n  labs(title = \"Strong regression on number of enrollment and teachers in 2019\", x=\"Enrollment\", y =\"Teachers\", subtitle = \"The higher number of students requires the higher number of teacher \")+\n  geom_label_repel(data=text_point, aes(x= `2019_enrollment`, y=`2019_teacher`, label= State), color = \"blue\", size = 2)\n\n\n\n\n\n\n\n\n\nThe mean of teacher ratio is 15.49 over times. One teacher take care 16 people in a class on average. The above graph shows that enrollment have a strong relationship with the number of teachers. school tend to hire more teachers when they have many children in a class. This means that school try to keep the teacher’s ratio."
  },
  {
    "objectID": "data_analysis/ds350/teacher_ratio.html#data-visualization-2-1-teacher-ratio-overtimes.",
    "href": "data_analysis/ds350/teacher_ratio.html#data-visualization-2-1-teacher-ratio-overtimes.",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 2-1 : teacher ratio overtimes.",
    "text": "Data & Visualization 2-1 : teacher ratio overtimes.\n\n\nCode\nteacher5&lt;- teacher1 %&gt;% \n  mutate(group_teacher = case_when(\n  `value` &lt; 12 ~ \"group1\",\n  `value` &lt; 13  & `value` &gt;= 12 ~ \"group2\",\n  `value` &lt; 14  & `value` &gt;= 13 ~ \"group3\",\n  `value` &lt; 15  & `value` &gt;= 14 ~ \"group4\",\n  `value` &gt;= 15   ~ \"group5\"\n)) %&gt;% \n  filter(year==\"2000\"| year ==\"2010\")\n\nggplot(teacher5, mapping =aes(x = value, y = state_abb)) +\n  geom_path(mapping = aes(group = state_abb), \n             arrow = arrow(length = unit(0.15, \"cm\")))+\n  geom_point(mapping =aes(color = year))+\n  theme(\n    plot.margin=unit(c( -7, -1, -4 , 1), 'cm'),\n    legend.position = \"left\"\n  )\n\n\n\n\n\n\n\n\n\nBetween 2000 and 2010, there was an improvement in teacher ratios across most states. Notably, FL, NH, and ND experienced the most significant improvements, with each teacher responsible for fewer students. However, in NV and IN, the number of students per teacher increased, indicating a decline in teacher ratios."
  },
  {
    "objectID": "data_analysis/ds350/teacher_ratio.html#data-visualization-3-teacher-ratio-by-state",
    "href": "data_analysis/ds350/teacher_ratio.html#data-visualization-3-teacher-ratio-by-state",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 3: Teacher Ratio by state",
    "text": "Data & Visualization 3: Teacher Ratio by state\n\n\nCode\n# ggplot(teacher1, aes(year, value))+\n#   geom_boxplot()+\n#   geom_jitter(aes(colour = State), width = 0.25)+\n#   theme(legend.position = \"none\")+\n#   geom_text_repel(teacher1, mapping = aes(label=state_abb))\n\n# ggplot(teacher1, aes(as.numeric(year), value)) +\n#   geom_line() +\n#   facet_geo(~ state_abb, grid = \"us_state_grid2\") +\n#   scale_x_continuous(labels = function(x) paste0(\"'\", substr(x, 3, 4))) +\n#   ylab(\"Teacher Rate (%)\")+\n#   theme_bw()+\n#   labs(title = \"Teacher ratio by State\", x=\"year\", y =\"Teacher ratio\", caption = \"This data starts from 2000 to 2019\")+\n#   theme(legend.position = \"none\",\n#         plot.title = element_text(face = \"bold\", family = \"serif\"),\n#         plot.caption = element_text(color =\"grey40\", face=\"italic\", angle =0.5))\n\nggplot(teacher1, aes(as.numeric(year), diff)) +\n  geom_bar(stat= \"identity\") +\n  geom_hline(yintercept = 0, color= \"orange\")+\n  facet_geo(~ state_abb, grid = \"us_state_grid2\") +\n  scale_x_continuous(labels = function(x) paste0(\"'\", substr(x, 3, 4)),\n                     breaks = c(2000, 2010, 2020)) +\n  ylab(\"Teacher Rate (%)\")+\n  theme_bw()+\n  labs(title = \"Teacher ratio by State\", x=\"year\", y =\"Teacher ratio\", caption = \"This data starts from 2000 to 2019\")+\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\", family = \"serif\"),\n        plot.caption = element_text(color =\"grey40\", face=\"italic\", angle =0.5),\n        strip.text.x = element_text(size = 6))\n\n\n\n\n\n\n\n\n\nThe graph illustrates the states that have a higher teacher ratio than the average. It is interesting to note that western states are more densely populated and have a higher teacher ratio, meaning that each teacher is responsible for a larger number of students. Conversely, eastern and northern states have a lower teacher ratio."
  },
  {
    "objectID": "data_analysis/ds350/teacher_ratio.html#data-visualization-3-2-teacher-ratio-by-state",
    "href": "data_analysis/ds350/teacher_ratio.html#data-visualization-3-2-teacher-ratio-by-state",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 3-2: teacher ratio by state",
    "text": "Data & Visualization 3-2: teacher ratio by state\n\n\nCode\nstate_shape &lt;- us_states() %&gt;% \n  filter(jurisdiction_type == \"state\", \n         state_abbr != \"AK\", \n         state_abbr != \"HI\")\nstate_shape$state_abbr\n\n\n [1] \"CA\" \"WI\" \"ID\" \"MN\" \"IA\" \"MO\" \"MD\" \"OR\" \"MI\" \"MT\" \"UT\" \"VA\" \"IL\" \"TX\" \"LA\"\n[16] \"FL\" \"KY\" \"TN\" \"NY\" \"GA\" \"DE\" \"WA\" \"OK\" \"WV\" \"PA\" \"CO\" \"NC\" \"IN\" \"KS\" \"RI\"\n[31] \"WY\" \"MS\" \"MA\" \"CT\" \"VT\" \"ME\" \"NH\" \"NM\" \"OH\" \"AL\" \"NE\" \"SD\" \"NV\" \"NJ\" \"ND\"\n[46] \"SC\" \"AZ\" \"AR\"\n\n\nCode\nteacher3&lt;- teacher2 %&gt;% mutate(group_edu = case_when(\n  `2019` &lt; 12 ~ \"group1\",\n  `2019` &lt; 13  & `2019` &gt;= 12 ~ \"group2\",\n  `2019` &lt; 14  & `2019` &gt;= 13 ~ \"group3\",\n  `2019` &lt; 15  & `2019` &gt;= 14 ~ \"group4\",\n  `2019` &gt;= 15   ~ \"group5\"\n)) %&gt;% \n  mutate(state_abbr = state.abb[match(State, state.name)]) %&gt;% \n  filter(state_abbr != \"AK\", \n         state_abbr != \"HI\") %&gt;% \n  arrange(`2019`)\n\nteacher4 &lt;- full_join(state_shape, teacher3, by =\"state_abbr\")\n\nggplot() +\n  geom_sf(data = state_shape)+\n  geom_sf(data= teacher4, aes(fill=group_edu, geometry = geometry),  stat = \"sf_coordinates\", alpha = 0.6)+\n  theme_minimal()+\n  theme(panel.border = element_rect(color=\"black\", fill =NA))+\n  theme(legend.position = \"bottom\")+\n  geom_sf_text(data= teacher4, aes(label = state_abbr), size = 2)\n\n\n\n\n\n\n\n\n\nThe graphs above illustrate the difference in teacher ratios from the average, categorized into five groups for values higher than the mean. It is evident from the graphs that Vermont has the lowest teacher ratio.\nQ. What is the reason east north have lower teacher ratio?\n\nData & Visualization 4: teacher salary by state\n\n\nCode\nteacher_salary &lt;- read_xls(\"C:/Users/gakyeong/Desktop/2022.4th/project/teacher salary.xls\", skip=2)\ncolnames(teacher_salary)[1] =\"State\"\n\nteacher_salary &lt;-teacher_salary %&gt;% \n  filter(!State %in% c('Puerto Rico', 'District of Columbia', \"United States\")) %&gt;% \n  select (State,\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\", \"9\") %&gt;% \n  group_by(State) %&gt;% \n  arrange(State) %&gt;% \n  mutate(state_abb = state.abb[match(State, state.name)]) %&gt;% \n  rename(\"1970\"=`2`,\"1980\"=`3`, \"1990\"=`4`, \"2000\"=`5`, \"2010\"=`6`, \"2020\" = `7`, \"2021\" =`8`, \"2022\" = `9`)\n\nTea_sal1 &lt;- teacher_salary %&gt;% \n  pivot_longer(cols= c('1970':'2022'), names_to = \"year\", values_to = \"value\" ,values_drop_na = TRUE)\n\nUS_avg &lt;-Tea_sal1 %&gt;% \n  group_by(year) %&gt;% \n  summarise(average = median(value))\n\nTop5_ratio&lt;- Tea_sal1  %&gt;% filter(state_abb %in% c(\"VT\", \"NJ\", \"NH\", \"ME\", \"CT\"))\nTop5_ratio1&lt;- Tea_sal1  %&gt;% filter(state_abb %in% c(\"VT\", \"NJ\", \"NH\", \"ME\", \"CT\"),\n                                   year == \"2022\")\n\nVT &lt;- Tea_sal1 %&gt;% filter(state_abb == \"VT\")\nNJ &lt;- Tea_sal1 %&gt;% filter(state_abb == \"NJ\")\nNH &lt;- Tea_sal1 %&gt;% filter(state_abb == \"NH\")\nME &lt;- Tea_sal1 %&gt;% filter(state_abb == \"ME\")\nCT &lt;- Tea_sal1 %&gt;% filter(state_abb == \"CT\")\n\noptions(scipen = 999)\nlibrary(scales)\n\nggplot()+\n  geom_point(data = Tea_sal1, aes(x= year, y=value),color=\"grey\")+\n  geom_point(data=US_avg, aes(x= year, y=average), size = 2.3)+\n  geom_segment(data = US_avg, aes(x = year,\n                                  y=average,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(average, n=-1), NA)),\n               size = 0.8)+\n  geom_point(data= Top5_ratio, aes(x= year, y=value), color=\"red\", fill= \"white\", size =1.5)+\n  geom_segment(data = VT, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  geom_segment(data = NJ, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  geom_segment(data = NH, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  geom_segment(data = ME, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  geom_segment(data = CT, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  theme_bw()+\n  theme(legend.position = \"none\")+\n  labs(title = \"History of teacher's salary by state\", y=\"Annual Salary ($)\")+\n  scale_y_continuous(labels = scales::comma)+\n  geom_text_repel(data= Top5_ratio1, \n                  aes(x= year, y=value, label= state_abb),\n                  direction = \"y\", \n                  hjust = -1)\n\n\n\n\n\n\n\n\n\nTo investigate the reason behind the lower teacher ratio in the east-northern states, I am interested in exploring the relationship between teacher ratio and teacher salary. The black line above represents the average salary of teachers in the US, indicating a sharp increase in annual salary between 1980 and 2010.\n\n\nData & Visualization 4-2: Teacher salary over time\n\n\nCode\nlibrary(leaflet)\n\nmax(Tea_sal1$value) - mean(Tea_sal1$value)\n\n\n[1] 51120.88\n\n\nCode\nTea_sal2 &lt;-Tea_sal1 %&gt;% \n  rename(\"state_abbr\"= state_abb) %&gt;% \n  filter(state_abbr != \"AK\", \n         state_abbr != \"HI\") %&gt;% \n  mutate(group_sal = case_when(\n  value &gt;= mean(Tea_sal1$value)+40000 ~ \"group1\",\n  value &lt; mean(Tea_sal1$value)+40000 & value &gt;= mean(Tea_sal1$value)+30000 ~ \"group2\",\n  value &lt; mean(Tea_sal1$value)+30000 & value &gt;= mean(Tea_sal1$value)+20000 ~ \"group3\",\n  value &lt; mean(Tea_sal1$value)+20000 & value &gt;= mean(Tea_sal1$value)+10000 ~ \"group4\",\n  value &lt; mean(Tea_sal1$value)+10000 & value &gt;= mean(Tea_sal1$value) ~ \"group5\", \n  value &lt; mean(Tea_sal1$value) ~\"group6\"\n))\n\nTea_sal3 &lt;- inner_join(state_shape, Tea_sal2, by=\"state_abbr\")\n\n\n\n\nCode\npal &lt;- colorNumeric(palette = c(\"white\",\"orange\", \"red\"),\n                    domain = min(Tea_sal3$value):max(Tea_sal3$value))\n\n\n\n\nCode\nleaflet() %&gt;% \n  setView(lng = -99, lat = 40, zoom = 4) %&gt;% \n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;%\n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"1970\")),\n              group = \"1970\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"1980\")),\n              group = \"1980\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"1990\")),\n              group = \"1990\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"2000\")),\n              group = \"2000\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"2010\")),\n              group = \"2010\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"2020\")),\n              group = \"2020\",\n              fillOpacity = .5,\n              fillColor = ~pal(value))%&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"2021\")),\n              group = \"2021\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addLayersControl(\n    baseGroups = c(\"1970\", \"1980\", \"1990\", \"2000\", \"2010\",\"2020\", \"2021\"),\n    options = layersControlOptions(collapsed = FALSE)) \n\n\n\n\n\n\nOver time, it has been demonstrated that teacher salaries have increased. However, teacher salary is not the only factor that affects teacher ratio. It has been observed that the eastern and northern states of CT, NJ, VT, and NH, which also have lower teacher ratios, offer higher annual salaries to teachers. Conversely, ME has a lower teacher ratio but offers the same salary as the average."
  },
  {
    "objectID": "data_analysis/ds350/teacher_ratio.html#conclusion",
    "href": "data_analysis/ds350/teacher_ratio.html#conclusion",
    "title": "Education Equality in teacher ratio",
    "section": "Conclusion",
    "text": "Conclusion\nIn general, there has been a decrease in teacher ratios, allowing students to receive more personalized education in a classroom setting. Notably, in the eastern and northern states, teacher salary appears to be related to teacher ratios to some extent, with states offering higher salaries experiencing lower ratios. However, further detailed data is needed to determine the other factors that influence the lower-than-average teacher ratios in these states."
  },
  {
    "objectID": "data_analysis/ds350/final project.html.html",
    "href": "data_analysis/ds350/final project.html.html",
    "title": "Education Equality in teacher ratio",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggrepel)\nlibrary(readxl)\nlibrary(stringr)\n# install.packages(\"geofacet\")\nlibrary(geofacet)\nlibrary(USAboundaries)\nlibrary(USAboundariesData)\nlibrary(sf)"
  },
  {
    "objectID": "data_analysis/ds350/final project.html.html#introduction",
    "href": "data_analysis/ds350/final project.html.html#introduction",
    "title": "Education Equality in teacher ratio",
    "section": "Introduction",
    "text": "Introduction\nEducation attainment refers to the highest level of education that an individual has completed. Inequalities in education is problematic in society level because education achievement usually leads to economic prospects. According to UNICEF, the lack of qualified teachers, inadequate teaching materials, and poor sanitation are some of the reasons why children do not receive a quality education. Since house income have been enough to support clean sanitation in US, we want to know more about teacher ratio to analysis inadequate teaching materials and lack of qualified teachers regarding the equality of education. All date comes from National Center for Education Statistics (NCES)."
  },
  {
    "objectID": "data_analysis/ds350/final project.html.html#data-visualization-1-school-drop-rate",
    "href": "data_analysis/ds350/final project.html.html#data-visualization-1-school-drop-rate",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 1 : school drop rate",
    "text": "Data & Visualization 1 : school drop rate\n\n\nCode\ndrop_data &lt;- read_xlsx(\"C:/Users/gakyeong/Desktop/2022.4th/project/drop out.xlsx\",sheet = 3) \n# rename columns, substr\ncolnames(drop_data)[1] =\"year\" ; colnames(drop_data)[2] = \"All_races\"\ncolnames(drop_data)[4] =\"White\" ; colnames(drop_data)[6] =\"Black\"; colnames(drop_data)[8] =\"Hispanic\"\n\ndrop_data$year &lt;- substr(drop_data$year, 0, 4)\n\ndrop_data1 &lt;- drop_data %&gt;% \n  select(year, All_races, White, Black, Hispanic) %&gt;% \n  filter(year &gt;=1975) %&gt;% \n  mutate(Hispanic = as.numeric(Hispanic), year = as.numeric(year))\n\n\n\n\nCode\nggplot()+\n  geom_line(data = drop_data1, aes(x =year, y= All_races), color = \"cyan3\", size =1.5)+\n  geom_line(data = drop_data1, aes(x =year, y= White), color = \"cornsilk\", size =1.5)+\n  geom_line(data = drop_data1, aes(x =year, y= Black),  color = \"darkgrey\", size =1.5)+\n  geom_line(data = drop_data1, aes(x =year, y= Hispanic),  color = \"darkgoldenrod1\", size =1.5)+\n  theme_bw()+\n  annotate(geom = \"text\", x = 2018, y = 2300, label = \"All races\", hjust = \"left\", color= \"cyan3\")+\n  labs(title = \"The Number of school drop by ethnicity\" , y=\"\")+\n  theme(plot.title = element_text(face = \"bold\", family = \"serif\"))\n\n\n\n\n\n\n\n\n\nIt can be concluded that as high school dropout rates decrease, there is an increase in educational attainment over time. Furthermore, since 2020, the number of school dropouts has decreased irrespective of the student’s ethnicity.\nIt makes me to wonders that not only decreasing number of drop out in secondary school, but also how students get the qualified education. For these reason, one of the assessment on education quality is that one teacher care how many students in a class."
  },
  {
    "objectID": "data_analysis/ds350/final project.html.html#data-visualization-2-general-teacher-ratio-information",
    "href": "data_analysis/ds350/final project.html.html#data-visualization-2-general-teacher-ratio-information",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 2: General teacher ratio information",
    "text": "Data & Visualization 2: General teacher ratio information\n\n\nCode\nteacher_ratio &lt;- read_xls(\"C:/Users/gakyeong/Desktop/2022.4th/project/teacher ratio.xls\", skip=2)\ncolnames(teacher_ratio)[1] =\"State\"\n\nteacher &lt;-teacher_ratio %&gt;% \n  filter(!State %in% c('Puerto Rico', 'District of Columbia', \"United States\")) %&gt;% \n  select (State,\"2\",\"3\",\"4\",\"5\",\"6\", \"9\", \"12\",\"13\",\"14\", \"15\") %&gt;% \n  group_by(State) %&gt;% \n  arrange(State) %&gt;% \n  mutate(state_abb = state.abb[match(State, state.name)]) %&gt;% \n  rename(\"2000\"=`2`,\"2005\"=`3`, \"2010\"=`4`, \"2015\"=`5`, \"2016\"=`6`, \"2017\" =`9`, \"2018\" = `12`, \"2019\" = `15`, \"2019_teacher\" = `13`, \"2019_enrollment\" = `14`) \n\nteacher1 &lt;- teacher %&gt;% \n  select(State, \"2000\",\"2005\", \"2010\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\") %&gt;% \n  mutate(state_abb = state.abb[match(State, state.name)]) %&gt;% \n  pivot_longer(cols= c('2000':'2019'), names_to = \"year\", values_to = \"value\" ,values_drop_na = TRUE)\n\nteacher1&lt;-teacher1 %&gt;% mutate(diff = (value - mean(teacher1$value)))\n\n\n\n\nCode\nteacher2 &lt;- teacher %&gt;% \n  select(State, \"2019_enrollment\", \"2019_teacher\", \"2019\" ) %&gt;% \n  arrange(`2019_enrollment`) %&gt;% \n  na.omit()\n\ntext_point &lt;- teacher2 %&gt;% filter(State %in% c(\"North Dakota\",\"West Virginia\", \"Vermont\", \"Connecticut\"))\n\nggplot(teacher2, aes(x= `2019_enrollment`, y=`2019_teacher`)) + \n  geom_point(aes(color = State))+\n  geom_smooth(method = \"lm\", se = FALSE, color= \"orange\")+\n  scale_x_continuous(trans = \"log\")+\n  scale_y_continuous(trans = \"log\")+\n  theme_bw()+\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\", family = \"serif\"),\n        plot.subtitle = element_text(color =\"grey40\", face=\"italic\", angle =0.5))+\n  labs(title = \"Strong regression on number of enrollment and teachers in 2019\", x=\"Enrollment\", y =\"Teachers\", subtitle = \"The higher number of students requires the higher number of teacher \")+\n  geom_label_repel(data=text_point, aes(x= `2019_enrollment`, y=`2019_teacher`, label= State), color = \"blue\", size = 2)\n\n\n\n\n\n\n\n\n\nThe mean of teacher ratio is 15.49 over times. One teacher take care 16 people in a class on average. The above graph shows that enrollment have a strong relationship with the number of teachers. school tend to hire more teachers when they have many children in a class. This means that school try to keep the teacher’s ratio."
  },
  {
    "objectID": "data_analysis/ds350/final project.html.html#data-visualization-2-1-teacher-ratio-overtimes.",
    "href": "data_analysis/ds350/final project.html.html#data-visualization-2-1-teacher-ratio-overtimes.",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 2-1 : teacher ratio overtimes.",
    "text": "Data & Visualization 2-1 : teacher ratio overtimes.\n\n\nCode\nteacher5&lt;- teacher1 %&gt;% \n  mutate(group_teacher = case_when(\n  `value` &lt; 12 ~ \"group1\",\n  `value` &lt; 13  & `value` &gt;= 12 ~ \"group2\",\n  `value` &lt; 14  & `value` &gt;= 13 ~ \"group3\",\n  `value` &lt; 15  & `value` &gt;= 14 ~ \"group4\",\n  `value` &gt;= 15   ~ \"group5\"\n)) %&gt;% \n  filter(year==\"2000\"| year ==\"2010\")\n\nggplot(teacher5, mapping =aes(x = value, y = state_abb)) +\n  geom_path(mapping = aes(group = state_abb), \n             arrow = arrow(length = unit(0.15, \"cm\")))+\n  geom_point(mapping =aes(color = year))+\n  theme(\n    plot.margin=unit(c( -7, -1, -4 , 1), 'cm'),\n    legend.position = \"left\"\n  )\n\n\n\n\n\n\n\n\n\nBetween 2000 and 2010, there was an improvement in teacher ratios across most states. Notably, FL, NH, and ND experienced the most significant improvements, with each teacher responsible for fewer students. However, in NV and IN, the number of students per teacher increased, indicating a decline in teacher ratios."
  },
  {
    "objectID": "data_analysis/ds350/final project.html.html#data-visualization-3-teacher-ratio-by-state",
    "href": "data_analysis/ds350/final project.html.html#data-visualization-3-teacher-ratio-by-state",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 3: Teacher Ratio by state",
    "text": "Data & Visualization 3: Teacher Ratio by state\n\n\nCode\n# ggplot(teacher1, aes(year, value))+\n#   geom_boxplot()+\n#   geom_jitter(aes(colour = State), width = 0.25)+\n#   theme(legend.position = \"none\")+\n#   geom_text_repel(teacher1, mapping = aes(label=state_abb))\n\n# ggplot(teacher1, aes(as.numeric(year), value)) +\n#   geom_line() +\n#   facet_geo(~ state_abb, grid = \"us_state_grid2\") +\n#   scale_x_continuous(labels = function(x) paste0(\"'\", substr(x, 3, 4))) +\n#   ylab(\"Teacher Rate (%)\")+\n#   theme_bw()+\n#   labs(title = \"Teacher ratio by State\", x=\"year\", y =\"Teacher ratio\", caption = \"This data starts from 2000 to 2019\")+\n#   theme(legend.position = \"none\",\n#         plot.title = element_text(face = \"bold\", family = \"serif\"),\n#         plot.caption = element_text(color =\"grey40\", face=\"italic\", angle =0.5))\n\nggplot(teacher1, aes(as.numeric(year), diff)) +\n  geom_bar(stat= \"identity\") +\n  geom_hline(yintercept = 0, color= \"orange\")+\n  facet_geo(~ state_abb, grid = \"us_state_grid2\") +\n  scale_x_continuous(labels = function(x) paste0(\"'\", substr(x, 3, 4)),\n                     breaks = c(2000, 2010, 2020)) +\n  ylab(\"Teacher Rate (%)\")+\n  theme_bw()+\n  labs(title = \"Teacher ratio by State\", x=\"year\", y =\"Teacher ratio\", caption = \"This data starts from 2000 to 2019\")+\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\", family = \"serif\"),\n        plot.caption = element_text(color =\"grey40\", face=\"italic\", angle =0.5),\n        strip.text.x = element_text(size = 6))\n\n\n\n\n\n\n\n\n\nThe graph illustrates the states that have a higher teacher ratio than the average. It is interesting to note that western states are more densely populated and have a higher teacher ratio, meaning that each teacher is responsible for a larger number of students. Conversely, eastern and northern states have a lower teacher ratio."
  },
  {
    "objectID": "data_analysis/ds350/final project.html.html#data-visualization-3-2-teacher-ratio-by-state",
    "href": "data_analysis/ds350/final project.html.html#data-visualization-3-2-teacher-ratio-by-state",
    "title": "Education Equality in teacher ratio",
    "section": "Data & Visualization 3-2: teacher ratio by state",
    "text": "Data & Visualization 3-2: teacher ratio by state\n\n\nCode\nstate_shape &lt;- us_states() %&gt;% \n  filter(jurisdiction_type == \"state\", \n         state_abbr != \"AK\", \n         state_abbr != \"HI\")\nstate_shape$state_abbr\n\n\n [1] \"CA\" \"WI\" \"ID\" \"MN\" \"IA\" \"MO\" \"MD\" \"OR\" \"MI\" \"MT\" \"UT\" \"VA\" \"IL\" \"TX\" \"LA\"\n[16] \"FL\" \"KY\" \"TN\" \"NY\" \"GA\" \"DE\" \"WA\" \"OK\" \"WV\" \"PA\" \"CO\" \"NC\" \"IN\" \"KS\" \"RI\"\n[31] \"WY\" \"MS\" \"MA\" \"CT\" \"VT\" \"ME\" \"NH\" \"NM\" \"OH\" \"AL\" \"NE\" \"SD\" \"NV\" \"NJ\" \"ND\"\n[46] \"SC\" \"AZ\" \"AR\"\n\n\nCode\nteacher3&lt;- teacher2 %&gt;% mutate(group_edu = case_when(\n  `2019` &lt; 12 ~ \"group1\",\n  `2019` &lt; 13  & `2019` &gt;= 12 ~ \"group2\",\n  `2019` &lt; 14  & `2019` &gt;= 13 ~ \"group3\",\n  `2019` &lt; 15  & `2019` &gt;= 14 ~ \"group4\",\n  `2019` &gt;= 15   ~ \"group5\"\n)) %&gt;% \n  mutate(state_abbr = state.abb[match(State, state.name)]) %&gt;% \n  filter(state_abbr != \"AK\", \n         state_abbr != \"HI\") %&gt;% \n  arrange(`2019`)\n\nteacher4 &lt;- full_join(state_shape, teacher3, by =\"state_abbr\")\n\nggplot() +\n  geom_sf(data = state_shape)+\n  geom_sf(data= teacher4, aes(fill=group_edu, geometry = geometry),  stat = \"sf_coordinates\", alpha = 0.6)+\n  theme_minimal()+\n  theme(panel.border = element_rect(color=\"black\", fill =NA))+\n  theme(legend.position = \"bottom\")+\n  geom_sf_text(data= teacher4, aes(label = state_abbr), size = 2)\n\n\n\n\n\n\n\n\n\nThe graphs above illustrate the difference in teacher ratios from the average, categorized into five groups for values higher than the mean. It is evident from the graphs that Vermont has the lowest teacher ratio.\nQ. What is the reason east north have lower teacher ratio?\n\nData & Visualization 4: teacher salary by state\n\n\nCode\nteacher_salary &lt;- read_xls(\"C:/Users/gakyeong/Desktop/2022.4th/project/teacher salary.xls\", skip=2)\ncolnames(teacher_salary)[1] =\"State\"\n\nteacher_salary &lt;-teacher_salary %&gt;% \n  filter(!State %in% c('Puerto Rico', 'District of Columbia', \"United States\")) %&gt;% \n  select (State,\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\", \"9\") %&gt;% \n  group_by(State) %&gt;% \n  arrange(State) %&gt;% \n  mutate(state_abb = state.abb[match(State, state.name)]) %&gt;% \n  rename(\"1970\"=`2`,\"1980\"=`3`, \"1990\"=`4`, \"2000\"=`5`, \"2010\"=`6`, \"2020\" = `7`, \"2021\" =`8`, \"2022\" = `9`)\n\nTea_sal1 &lt;- teacher_salary %&gt;% \n  pivot_longer(cols= c('1970':'2022'), names_to = \"year\", values_to = \"value\" ,values_drop_na = TRUE)\n\nUS_avg &lt;-Tea_sal1 %&gt;% \n  group_by(year) %&gt;% \n  summarise(average = median(value))\n\nTop5_ratio&lt;- Tea_sal1  %&gt;% filter(state_abb %in% c(\"VT\", \"NJ\", \"NH\", \"ME\", \"CT\"))\nTop5_ratio1&lt;- Tea_sal1  %&gt;% filter(state_abb %in% c(\"VT\", \"NJ\", \"NH\", \"ME\", \"CT\"),\n                                   year == \"2022\")\n\nVT &lt;- Tea_sal1 %&gt;% filter(state_abb == \"VT\")\nNJ &lt;- Tea_sal1 %&gt;% filter(state_abb == \"NJ\")\nNH &lt;- Tea_sal1 %&gt;% filter(state_abb == \"NH\")\nME &lt;- Tea_sal1 %&gt;% filter(state_abb == \"ME\")\nCT &lt;- Tea_sal1 %&gt;% filter(state_abb == \"CT\")\n\noptions(scipen = 999)\nlibrary(scales)\n\nggplot()+\n  geom_point(data = Tea_sal1, aes(x= year, y=value),color=\"grey\")+\n  geom_point(data=US_avg, aes(x= year, y=average), size = 2.3)+\n  geom_segment(data = US_avg, aes(x = year,\n                                  y=average,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(average, n=-1), NA)),\n               size = 0.8)+\n  geom_point(data= Top5_ratio, aes(x= year, y=value), color=\"red\", fill= \"white\", size =1.5)+\n  geom_segment(data = VT, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  geom_segment(data = NJ, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  geom_segment(data = NH, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  geom_segment(data = ME, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  geom_segment(data = CT, aes(x = year,\n                                  y=value,\n                                  xend=c(tail(year, n=-1), NA), \n                                  yend=c(tail(value, n=-1), NA)),\n               linetype = \"dotted\")+\n  theme_bw()+\n  theme(legend.position = \"none\")+\n  labs(title = \"History of teacher's salary by state\", y=\"Annual Salary ($)\")+\n  scale_y_continuous(labels = scales::comma)+\n  geom_text_repel(data= Top5_ratio1, \n                  aes(x= year, y=value, label= state_abb),\n                  direction = \"y\", \n                  hjust = -1)\n\n\n\n\n\n\n\n\n\nTo investigate the reason behind the lower teacher ratio in the east-northern states, I am interested in exploring the relationship between teacher ratio and teacher salary. The black line above represents the average salary of teachers in the US, indicating a sharp increase in annual salary between 1980 and 2010.\n\n\nData & Visualization 4-2: Teacher salary over time\n\n\nCode\nlibrary(leaflet)\n\nmax(Tea_sal1$value) - mean(Tea_sal1$value)\n\n\n[1] 51120.88\n\n\nCode\nTea_sal2 &lt;-Tea_sal1 %&gt;% \n  rename(\"state_abbr\"= state_abb) %&gt;% \n  filter(state_abbr != \"AK\", \n         state_abbr != \"HI\") %&gt;% \n  mutate(group_sal = case_when(\n  value &gt;= mean(Tea_sal1$value)+40000 ~ \"group1\",\n  value &lt; mean(Tea_sal1$value)+40000 & value &gt;= mean(Tea_sal1$value)+30000 ~ \"group2\",\n  value &lt; mean(Tea_sal1$value)+30000 & value &gt;= mean(Tea_sal1$value)+20000 ~ \"group3\",\n  value &lt; mean(Tea_sal1$value)+20000 & value &gt;= mean(Tea_sal1$value)+10000 ~ \"group4\",\n  value &lt; mean(Tea_sal1$value)+10000 & value &gt;= mean(Tea_sal1$value) ~ \"group5\", \n  value &lt; mean(Tea_sal1$value) ~\"group6\"\n))\n\nTea_sal3 &lt;- inner_join(state_shape, Tea_sal2, by=\"state_abbr\")\n\n\n\n\nCode\npal &lt;- colorNumeric(palette = c(\"white\",\"orange\", \"red\"),\n                    domain = min(Tea_sal3$value):max(Tea_sal3$value))\n\n\n\n\nCode\nleaflet() %&gt;% \n  setView(lng = -99, lat = 40, zoom = 4) %&gt;% \n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;%\n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"1970\")),\n              group = \"1970\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"1980\")),\n              group = \"1980\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"1990\")),\n              group = \"1990\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"2000\")),\n              group = \"2000\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"2010\")),\n              group = \"2010\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"2020\")),\n              group = \"2020\",\n              fillOpacity = .5,\n              fillColor = ~pal(value))%&gt;% \n  addPolygons(data = st_as_sf(filter(Tea_sal3, year == \"2021\")),\n              group = \"2021\",\n              fillOpacity = .5,\n              fillColor = ~pal(value)) %&gt;% \n  addLayersControl(\n    baseGroups = c(\"1970\", \"1980\", \"1990\", \"2000\", \"2010\",\"2020\", \"2021\"),\n    options = layersControlOptions(collapsed = FALSE)) \n\n\n\n\n\n\nOver time, it has been demonstrated that teacher salaries have increased. However, teacher salary is not the only factor that affects teacher ratio. It has been observed that the eastern and northern states of CT, NJ, VT, and NH, which also have lower teacher ratios, offer higher annual salaries to teachers. Conversely, ME has a lower teacher ratio but offers the same salary as the average."
  },
  {
    "objectID": "data_analysis/ds350/final project.html.html#conclusion",
    "href": "data_analysis/ds350/final project.html.html#conclusion",
    "title": "Education Equality in teacher ratio",
    "section": "Conclusion",
    "text": "Conclusion\nIn general, there has been a decrease in teacher ratios, allowing students to receive more personalized education in a classroom setting. Notably, in the eastern and northern states, teacher salary appears to be related to teacher ratios to some extent, with states offering higher salaries experiencing lower ratios. However, further detailed data is needed to determine the other factors that influence the lower-than-average teacher ratios in these states."
  },
  {
    "objectID": "statistics/math425/final project.html",
    "href": "statistics/math425/final project.html",
    "title": "The Impact on Graduation Rates",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "statistics/math425/MyLogisticRegression.html",
    "href": "statistics/math425/MyLogisticRegression.html",
    "title": "Logistic Regression for Stock Volumn",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "statistics/math425/final_project.html",
    "href": "statistics/math425/final_project.html",
    "title": "The Impact on Graduation Rates",
    "section": "",
    "text": "Back to top"
  }
]